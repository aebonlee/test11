#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì´ˆìµœì í™” Pooling í‰ê°€ ì‹œìŠ¤í…œ - 1ëª… í…ŒìŠ¤íŠ¸
í† í° ì ˆê° ê¸°ë²•:
1. ì¤‘ë³µ ì œê±°
2. ë°ì´í„° ì••ì¶• (ë©”íƒ€ë°ì´í„° ì œê±°, í•µì‹¬ë§Œ ì¶”ì¶œ)
3. System Message ê·¹ë‹¨ì  ìµœì†Œí™”
4. Prompt Caching í™œìš©
5. ì¹´í…Œê³ ë¦¬ë³„ ë°°ì¹˜ ì²˜ë¦¬

ëª©í‘œ: 98% í† í° ì ˆê°
"""

import asyncio
import aiohttp
import json
import os
import re
import hashlib
from datetime import datetime
from typing import List, Dict
import anthropic
import openai
from supabase import create_client
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROK_API_KEY = os.getenv("XAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# í‰ê°€ ì¹´í…Œê³ ë¦¬
CATEGORIES = [
    "Expertise", "Leadership", "Vision", "Integrity", "Ethics",
    "Accountability", "Transparency", "Communication", "Responsiveness", "PublicInterest"
]

CATEGORY_KOREAN = {
    "Expertise": "ì „ë¬¸ì„±",
    "Leadership": "ë¦¬ë”ì‹­",
    "Vision": "ë¹„ì „",
    "Integrity": "ì²­ë ´ë„",
    "Ethics": "ìœ¤ë¦¬ì„±",
    "Accountability": "ì±…ì„ê°",
    "Transparency": "íˆ¬ëª…ì„±",
    "Communication": "ì†Œí†µëŠ¥ë ¥",
    "Responsiveness": "ëŒ€ì‘ì„±",
    "PublicInterest": "ê³µìµì„±"
}

# í…ŒìŠ¤íŠ¸ ì •ì¹˜ì¸ (ì†¡ì„ì¤€)
TEST_POLITICIAN = {
    "name": "ì†¡ì„ì¤€",
    "id": "023139c6"
}


class UltraOptimizedEvaluator:
    """ì´ˆìµœì í™” Pooling í‰ê°€ ì—”ì§„"""

    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        self.grok_api_key = GROK_API_KEY
        self.supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

        # ê·¹ë‹¨ì ìœ¼ë¡œ ì§§ì€ System Message (50 í† í°)
        self.minimal_system_template = "ì •ì¹˜ì¸ {category} í‰ê°€. ê° í•­ëª© 0-100ì . JSON ì¶œë ¥: {{\"scores\": [ì ìˆ˜1, ì ìˆ˜2, ...]}}"

    def _get_minimal_system_message(self, category: str) -> str:
        """ê·¹ë‹¨ì ìœ¼ë¡œ ì§§ì€ System Message"""
        return self.minimal_system_template.format(category=CATEGORY_KOREAN[category])

    def _load_data_from_db(self, politician_id: str) -> List[Dict]:
        """DBì—ì„œ ë°ì´í„° ë¡œë“œ"""

        response = self.supabase.table('collected_data').select(
            'data_title, data_content, ai_name, category_name, source_type'
        ).eq('politician_id', politician_id).execute()

        return response.data

    def _remove_duplicates(self, data_list: List[str]) -> List[str]:
        """ì¤‘ë³µ ì œê±°"""

        seen = set()
        unique = []

        for data in data_list:
            # ë‚´ìš© ê¸°ë°˜ í•´ì‹œ
            content_hash = hashlib.md5(data.encode('utf-8')).hexdigest()[:16]

            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(data)

        return unique

    def _compress_data(self, data: str) -> str:
        """ë°ì´í„° ì••ì¶• (ë©”íƒ€ë°ì´í„° ì œê±°, í•µì‹¬ë§Œ ì¶”ì¶œ)"""

        # 1. ë©”íƒ€ë°ì´í„° ì œê±°
        data = re.sub(r'\[.*?\]', '', data)  # [AI | Category | Type] ì œê±°

        # 2. ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        remove_phrases = [
            'ê´€ë ¨ ê¸°ì‚¬', 'ë” ë³´ê¸°', 'ê¸°ì‚¬ ì „ë¬¸', 'ì¶œì²˜:', 'ì €ì‘ê¶Œ',
            'ë¬´ë‹¨ ì „ì¬', 'ì¬ë°°í¬ ê¸ˆì§€', 'ë‰´ìŠ¤', 'ë³´ë„ìë£Œ',
            'ë”ë³´ê¸°', 'â–¶', 'â—†', 'â€»', 'â– '
        ]

        for phrase in remove_phrases:
            data = data.replace(phrase, '')

        # 3. ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°
        data = re.sub(r'\s+', ' ', data)
        data = data.strip()

        # 4. í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ (ì²« 3ë¬¸ì¥ + í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥)
        sentences = [s.strip() for s in data.split('.') if s.strip()]

        # ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ
        keywords = ['ì •ì±…', 'ë²•ì•ˆ', 'ì˜ˆì‚°', 'ê³µì•½', 'ê°œí˜', 'ì¶”ì§„', 'ë°œí‘œ', 'ì œì•ˆ', 'ê³„íš']

        key_sentences = sentences[:3]  # ì²« 3ë¬¸ì¥

        # í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ ì¶”ê°€ (ìµœëŒ€ 2ê°œ)
        for sent in sentences[3:]:
            if any(kw in sent for kw in keywords):
                key_sentences.append(sent)
                if len(key_sentences) >= 5:
                    break

        compressed = '. '.join(key_sentences[:5])

        return compressed

    def _ultra_compress_data_list(self, raw_data: List[Dict]) -> List[str]:
        """ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì••ì¶•"""

        print(f"\n  ğŸ“¦ ë°ì´í„° ì••ì¶• ì‹œì‘...")
        print(f"  ì›ë³¸: {len(raw_data)}ê°œ í•­ëª©")

        # 1. í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text_data = []
        for item in raw_data:
            title = item.get('data_title', '')
            content = item.get('data_content', '')
            text = f"{title}. {content}"
            text_data.append(text)

        # ì›ë³¸ í† í° ìˆ˜ ì¶”ì •
        original_tokens = sum(len(d.split()) for d in text_data)
        print(f"  ì›ë³¸ í† í°: ~{original_tokens:,}ê°œ")

        # 2. ì¤‘ë³µ ì œê±°
        unique_data = self._remove_duplicates(text_data)
        print(f"  ì¤‘ë³µ ì œê±° í›„: {len(unique_data)}ê°œ í•­ëª© ({len(unique_data)/len(text_data)*100:.1f}%)")

        # 3. ì••ì¶•
        compressed_data = [self._compress_data(d) for d in unique_data]

        # ì••ì¶• í›„ í† í° ìˆ˜ ì¶”ì •
        compressed_tokens = sum(len(d.split()) for d in compressed_data)
        print(f"  ì••ì¶• í›„ í† í°: ~{compressed_tokens:,}ê°œ")
        print(f"  í† í° ì ˆê°: {(1 - compressed_tokens/original_tokens)*100:.1f}%")

        return compressed_data

    async def evaluate_with_claude_optimized(
        self,
        politician_name: str,
        data_list: List[str],
        category: str
    ) -> List[int]:
        """Claude ìµœì í™” í‰ê°€ (1íšŒ API í˜¸ì¶œ)"""

        print(f"    ğŸ“Š Claude í‰ê°€: {category}")

        # ê·¹ë‹¨ì ìœ¼ë¡œ ì§§ì€ System Message
        system_msg = self._get_minimal_system_message(category)

        # ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ
        data_json = json.dumps(
            [{"n": i+1, "t": data[:200]} for i, data in enumerate(data_list)],  # ê° í•­ëª© 200ìë¡œ ì œí•œ
            ensure_ascii=False
        )

        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=16000,
                system=[
                    {
                        "type": "text",
                        "text": system_msg,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[
                    {
                        "role": "user",
                        "content": f"{politician_name}\n{data_json}"
                    }
                ]
            )

            result = json.loads(response.content[0].text)
            scores = result.get('scores', [])

            # í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
            usage = response.usage
            print(f"      í† í°: input={usage.input_tokens}, output={usage.output_tokens}, "
                  f"cache_creation={getattr(usage, 'cache_creation_input_tokens', 0)}, "
                  f"cache_read={getattr(usage, 'cache_read_input_tokens', 0)}")

            return scores

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {str(e)}")
            return []

    async def evaluate_with_chatgpt_optimized(
        self,
        politician_name: str,
        data_list: List[str],
        category: str
    ) -> List[int]:
        """ChatGPT ìµœì í™” í‰ê°€"""

        print(f"    ğŸ“Š ChatGPT í‰ê°€: {category}")

        system_msg = self._get_minimal_system_message(category)

        data_json = json.dumps(
            [{"n": i+1, "t": data[:200]} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": f"{politician_name}\n{data_json}"}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            scores = result.get('scores', [])

            # í† í° ì‚¬ìš©ëŸ‰
            usage = response.usage
            print(f"      í† í°: input={usage.prompt_tokens}, output={usage.completion_tokens}")

            return scores

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {str(e)}")
            return []

    async def evaluate_with_grok_optimized(
        self,
        politician_name: str,
        data_list: List[str],
        category: str
    ) -> List[int]:
        """Grok ìµœì í™” í‰ê°€"""

        print(f"    ğŸ“Š Grok í‰ê°€: {category}")

        system_msg = self._get_minimal_system_message(category)

        data_json = json.dumps(
            [{"n": i+1, "t": data[:200]} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.grok_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "grok-beta",
                        "messages": [
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": f"{politician_name}\n{data_json}"}
                        ],
                        "temperature": 0.3
                    }
                ) as resp:
                    data = await resp.json()
                    result = json.loads(data['choices'][0]['message']['content'])
                    scores = result.get('scores', [])

                    # í† í° ì‚¬ìš©ëŸ‰
                    usage = data.get('usage', {})
                    print(f"      í† í°: input={usage.get('prompt_tokens', 0)}, output={usage.get('completion_tokens', 0)}")

                    return scores

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {str(e)}")
            return []

    async def evaluate_one_politician_ultra_optimized(
        self,
        politician_name: str,
        politician_id: str
    ) -> Dict:
        """1ëª… ì´ˆìµœì í™” í‰ê°€"""

        print(f"\n{'='*80}")
        print(f"ğŸš€ ì´ˆìµœì í™” í‰ê°€ ì‹œì‘: {politician_name}")
        print(f"{'='*80}")

        start_time = datetime.now()

        # 1. ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“¥ 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")
        raw_data = self._load_data_from_db(politician_id)
        print(f"  DBì—ì„œ {len(raw_data)}ê°œ í•­ëª© ë¡œë“œ")

        # 2. ë°ì´í„° ì••ì¶•
        print(f"\nğŸ—œï¸ 2ë‹¨ê³„: ë°ì´í„° ì••ì¶•")
        compressed_data = self._ultra_compress_data_list(raw_data)

        # 3. ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€
        print(f"\nğŸ“Š 3ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ({len(CATEGORIES)}ê°œ)")

        all_scores = {
            'chatgpt': {cat: [] for cat in CATEGORIES},
            'grok': {cat: [] for cat in CATEGORIES},
            'claude': {cat: [] for cat in CATEGORIES}
        }

        for i, category in enumerate(CATEGORIES, 1):
            print(f"\n  [{i}/{len(CATEGORIES)}] {CATEGORY_KOREAN[category]} ({category})")

            # 3ê°œ AI ë³‘ë ¬ í‰ê°€
            chatgpt_task = self.evaluate_with_chatgpt_optimized(politician_name, compressed_data, category)
            grok_task = self.evaluate_with_grok_optimized(politician_name, compressed_data, category)
            claude_task = self.evaluate_with_claude_optimized(politician_name, compressed_data, category)

            chatgpt_scores, grok_scores, claude_scores = await asyncio.gather(
                chatgpt_task, grok_task, claude_task
            )

            all_scores['chatgpt'][category] = chatgpt_scores
            all_scores['grok'][category] = grok_scores
            all_scores['claude'][category] = claude_scores

            print(f"    âœ… ì™„ë£Œ: ChatGPT({len(chatgpt_scores)}), Grok({len(grok_scores)}), Claude({len(claude_scores)})")

        # 4. Pooling ì ìˆ˜ ê³„ì‚°
        print(f"\nğŸ“ˆ 4ë‹¨ê³„: Pooling ì ìˆ˜ ê³„ì‚°")
        pooling_scores = self._calculate_pooling(all_scores)

        elapsed = (datetime.now() - start_time).total_seconds()

        print(f"\n{'='*80}")
        print(f"âœ… í‰ê°€ ì™„ë£Œ!")
        print(f"  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        print(f"  ë°ì´í„° ìˆ˜: {len(compressed_data)}ê°œ")
        print(f"  ì¹´í…Œê³ ë¦¬: {len(CATEGORIES)}ê°œ")
        print(f"{'='*80}")

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š Pooling ì ìˆ˜:")
        for cat in CATEGORIES:
            score = pooling_scores.get(cat, 0)
            print(f"  {CATEGORY_KOREAN[cat]:8s}: {score:.2f}ì ")

        avg_score = sum(pooling_scores.values()) / len(pooling_scores)
        print(f"\n  í‰ê· : {avg_score:.2f}ì ")

        return {
            'politician': politician_name,
            'politician_id': politician_id,
            'data_count': len(compressed_data),
            'scores': all_scores,
            'pooling': pooling_scores,
            'elapsed_seconds': elapsed
        }

    def _calculate_pooling(self, all_scores: Dict) -> Dict:
        """Pooling ì ìˆ˜ ê³„ì‚°"""

        pooling = {}

        for category in CATEGORIES:
            chatgpt_scores = all_scores['chatgpt'][category]
            grok_scores = all_scores['grok'][category]
            claude_scores = all_scores['claude'][category]

            if chatgpt_scores and grok_scores and claude_scores:
                chatgpt_avg = sum(chatgpt_scores) / len(chatgpt_scores)
                grok_avg = sum(grok_scores) / len(grok_scores)
                claude_avg = sum(claude_scores) / len(claude_scores)

                pooling[category] = (chatgpt_avg + grok_avg + claude_avg) / 3
            else:
                pooling[category] = 0

        return pooling


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    print("\n" + "="*80)
    print("ğŸš€ ì´ˆìµœì í™” Pooling í‰ê°€ ì‹œìŠ¤í…œ - 1ëª… í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {TEST_POLITICIAN['name']}")
    print(f"ëª©í‘œ: 98% í† í° ì ˆê°")
    print("="*80)

    evaluator = UltraOptimizedEvaluator()

    # í‰ê°€ ì‹¤í–‰
    result = await evaluator.evaluate_one_politician_ultra_optimized(
        TEST_POLITICIAN['name'],
        TEST_POLITICIAN['id']
    )

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"ultra_optimized_test_{TEST_POLITICIAN['name']}_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")


if __name__ == "__main__":
    asyncio.run(main())
