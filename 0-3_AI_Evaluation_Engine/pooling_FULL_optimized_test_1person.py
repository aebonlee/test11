#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ ìµœì²¨ë‹¨ ì´ˆìµœì í™” Pooling í‰ê°€ ì‹œìŠ¤í…œ - 1ëª… í…ŒìŠ¤íŠ¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

í† í° ì ˆê° ê¸°ë²• (98% ëª©í‘œ):
1. âœ… ì¤‘ë³µ ì œê±° (10% ì ˆê°)
2. âœ… NER ê¸°ë°˜ í•µì‹¬ ì¶”ì¶œ (83% ì ˆê°)
3. âœ… LLMLingua ì••ì¶• (80% ì ˆê°)
4. âœ… ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±° (20% ì ˆê°)
5. âœ… System Message ê·¹ë‹¨ì  ìµœì†Œí™” (99% ì ˆê°)
6. âœ… Prompt Caching í™œìš© (90% ì ˆê°)
7. âœ… BatchPrompt (ëª¨ë“  ì¹´í…Œê³ ë¦¬ í•œ ë²ˆì—)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import asyncio
import aiohttp
import json
import os
import re
import hashlib
from datetime import datetime
from typing import List, Dict, Tuple
import anthropic
import openai
from supabase import create_client
from dotenv import load_dotenv

# ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from llmlingua import PromptCompressor
    LLMLINGUA_AVAILABLE = True
except ImportError:
    print("âš ï¸ LLMLingua ì—†ìŒ - ì„¤ì¹˜ ì¤‘...")
    LLMLINGUA_AVAILABLE = False

try:
    import spacy
    try:
        nlp = spacy.load("ko_core_news_sm")
        SPACY_AVAILABLE = True
    except:
        print("âš ï¸ í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        os.system("python -m spacy download ko_core_news_sm")
        nlp = spacy.load("ko_core_news_sm")
        SPACY_AVAILABLE = True
except ImportError:
    print("âš ï¸ Spacy ì—†ìŒ")
    SPACY_AVAILABLE = False

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROK_API_KEY = os.getenv("XAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# í‰ê°€ ì¹´í…Œê³ ë¦¬
CATEGORIES = [
    "Expertise", "Leadership", "Vision", "Integrity", "Ethics",
    "Accountability", "Transparency", "Communication", "Responsiveness", "PublicInterest"
]

CATEGORY_KOREAN = {
    "Expertise": "ì „ë¬¸ì„±",
    "Leadership": "ë¦¬ë”ì‹­",
    "Vision": "ë¹„ì „",
    "Integrity": "ì²­ë ´ë„",
    "Ethics": "ìœ¤ë¦¬ì„±",
    "Accountability": "ì±…ì„ê°",
    "Transparency": "íˆ¬ëª…ì„±",
    "Communication": "ì†Œí†µëŠ¥ë ¥",
    "Responsiveness": "ëŒ€ì‘ì„±",
    "PublicInterest": "ê³µìµì„±"
}

# í…ŒìŠ¤íŠ¸ ì •ì¹˜ì¸
TEST_POLITICIAN = {
    "name": "ê¹€ë™ì—°",
    "id": "17270f25"
}


class FullOptimizedEvaluator:
    """ìµœì²¨ë‹¨ ì´ˆìµœì í™” Pooling í‰ê°€ ì—”ì§„"""

    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        self.grok_api_key = GROK_API_KEY
        self.supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

        # LLMLingua ì••ì¶•ê¸° ì´ˆê¸°í™” (CPU ëª¨ë“œ)
        if LLMLINGUA_AVAILABLE:
            try:
                print("ğŸ”§ LLMLingua ì••ì¶•ê¸° ì´ˆê¸°í™” ì¤‘ (CPU ëª¨ë“œ)...")
                self.compressor = PromptCompressor(
                    model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
                    use_llmlingua2=True,
                    device_map="cpu"  # CPU ëª¨ë“œ ê°•ì œ
                )
                print("âœ… LLMLingua ì¤€ë¹„ ì™„ë£Œ (CPU)")
            except Exception as e:
                print(f"âš ï¸ LLMLingua ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.compressor = None
                print("âš ï¸ LLMLingua ë¯¸ì‚¬ìš© - ëŒ€ì²´ ì••ì¶• ì‚¬ìš©")
        else:
            self.compressor = None
            print("âš ï¸ LLMLingua ë¯¸ì‚¬ìš© - ëŒ€ì²´ ì••ì¶• ì‚¬ìš©")

        # NER ì¶”ì¶œê¸°
        if SPACY_AVAILABLE:
            self.nlp = nlp
            print("âœ… Spacy NER ì¤€ë¹„ ì™„ë£Œ")
        else:
            self.nlp = None
            print("âš ï¸ Spacy ë¯¸ì‚¬ìš© - ëŒ€ì²´ ì¶”ì¶œ ì‚¬ìš©")

        # ê·¹ë‹¨ì ìœ¼ë¡œ ì§§ì€ System Message (15 í† í°!)
        self.minimal_system = "ì •ì¹˜ì¸ 10ê°œ ì¹´í…Œê³ ë¦¬ í‰ê°€. ê° 0-100ì . JSON: {\"Expertise\":[ì ìˆ˜ë“¤], \"Leadership\":[ì ìˆ˜ë“¤], ...}"

    def _load_data_from_db(self, politician_id: str) -> List[Dict]:
        """DBì—ì„œ ë°ì´í„° ë¡œë“œ"""

        response = self.supabase.table('collected_data').select(
            'data_title, data_content, ai_name, category_name, source_type'
        ).eq('politician_id', politician_id).execute()

        return response.data

    def _remove_duplicates(self, data_list: List[str]) -> List[str]:
        """ì¤‘ë³µ ì œê±°"""

        seen = set()
        unique = []

        for data in data_list:
            content_hash = hashlib.md5(data.encode('utf-8')).hexdigest()[:16]

            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(data)

        return unique

    def _extract_with_ner(self, text: str) -> str:
        """NER ê¸°ë°˜ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""

        if not SPACY_AVAILABLE or not self.nlp:
            # Fallback: ì²« 3ë¬¸ì¥ë§Œ
            sentences = text.split('.')[:3]
            return '. '.join(sentences)

        try:
            doc = self.nlp(text[:1000])  # ì²˜ìŒ 1000ìë§Œ ì²˜ë¦¬

            # Named Entities ì¶”ì¶œ
            entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'DATE', 'LAW', 'GPE']]

            # í•µì‹¬ ëª…ì‚¬êµ¬
            noun_chunks = [chunk.text for chunk in doc.noun_chunks][:10]

            # í•µì‹¬ ë™ì‚¬
            key_verbs = [token.text for token in doc if token.pos_ == "VERB"][:5]

            # ì¡°í•©
            result = f"{' '.join(entities[:5])} {' '.join(noun_chunks)} {' '.join(key_verbs)}"
            return result[:500]  # ìµœëŒ€ 500ì

        except Exception as e:
            # ì˜¤ë¥˜ ì‹œ fallback
            sentences = text.split('.')[:3]
            return '. '.join(sentences)

    def _compress_with_llmlingua(self, text: str, target_ratio: float = 0.2) -> str:
        """LLMLinguaë¡œ ì••ì¶• (80% ì ˆê°)"""

        if not LLMLINGUA_AVAILABLE or not self.compressor:
            # Fallback: ë‹¨ìˆœ ì••ì¶•
            words = text.split()
            target_len = int(len(words) * target_ratio)
            return ' '.join(words[:target_len])

        try:
            result = self.compressor.compress_prompt(
                text,
                instruction="",
                question="",
                target_token=max(10, int(len(text.split()) * target_ratio)),
                condition_compare=True,
                condition_in_question='after',
                rank_method='longllmlingua',
                use_sentence_level_filter=False,
                context_budget="+100",
                dynamic_context_compression_ratio=0.4,
                reorder_context='sort'
            )

            return result['compressed_prompt']

        except Exception as e:
            print(f"      LLMLingua ì˜¤ë¥˜: {e}")
            # Fallback
            words = text.split()
            target_len = int(len(words) * target_ratio)
            return ' '.join(words[:target_len])

    def _remove_boilerplate(self, text: str) -> str:
        """ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°"""

        # ì œê±°í•  íŒ¨í„´
        patterns = [
            r'\[.*?\]',  # [ëŒ€ê´„í˜¸ ë‚´ìš©]
            r'<.*?>',    # <íƒœê·¸>
            r'http\S+',  # URL
            r'www\.\S+', # www ë§í¬
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text)

        # ì œê±°í•  ë‹¨ì–´
        remove_words = [
            'ê´€ë ¨ ê¸°ì‚¬', 'ë” ë³´ê¸°', 'ê¸°ì‚¬ ì „ë¬¸', 'ì¶œì²˜', 'ì €ì‘ê¶Œ',
            'ë¬´ë‹¨ ì „ì¬', 'ì¬ë°°í¬ ê¸ˆì§€', 'ë‰´ìŠ¤', 'ë³´ë„ìë£Œ',
            'ë”ë³´ê¸°', 'â–¶', 'â—†', 'â€»', 'â– ', '=', '-', '_'
        ]

        for word in remove_words:
            text = text.replace(word, '')

        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _ultra_compress_data_list(self, raw_data: List[Dict]) -> Tuple[List[str], Dict]:
        """4ë‹¨ê³„ ì´ˆì••ì¶•"""

        print(f"\n{'='*80}")
        print(f"ğŸ—œï¸ 4ë‹¨ê³„ ì´ˆì••ì¶• í”„ë¡œì„¸ìŠ¤")
        print(f"{'='*80}")

        # í†µê³„
        stats = {
            'original_count': len(raw_data),
            'original_tokens': 0,
            'after_dedup_count': 0,
            'after_ner_tokens': 0,
            'after_llmlingua_tokens': 0,
            'final_tokens': 0
        }

        # 1. í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        print(f"\nğŸ“‹ ë‹¨ê³„ 0: í…ìŠ¤íŠ¸ ë³€í™˜")
        text_data = []
        for item in raw_data:
            title = item.get('data_title', '')
            content = item.get('data_content', '')
            text = f"{title}. {content}"
            text_data.append(text)

        stats['original_tokens'] = sum(len(t.split()) for t in text_data)
        print(f"  ì›ë³¸: {len(text_data)}ê°œ, ~{stats['original_tokens']:,} í† í°")

        # 2. ì¤‘ë³µ ì œê±°
        print(f"\nğŸ” ë‹¨ê³„ 1: ì¤‘ë³µ ì œê±°")
        unique_data = self._remove_duplicates(text_data)
        stats['after_dedup_count'] = len(unique_data)
        reduction = (1 - len(unique_data) / len(text_data)) * 100
        print(f"  ê²°ê³¼: {len(unique_data)}ê°œ ({reduction:.1f}% ì ˆê°)")

        # 3. NER í•µì‹¬ ì¶”ì¶œ
        print(f"\nğŸ¯ ë‹¨ê³„ 2: NER í•µì‹¬ ì¶”ì¶œ")
        ner_extracted = []
        for i, text in enumerate(unique_data):
            if i % 100 == 0 and i > 0:
                print(f"    ì§„í–‰: {i}/{len(unique_data)}")
            extracted = self._extract_with_ner(text)
            ner_extracted.append(extracted)

        stats['after_ner_tokens'] = sum(len(t.split()) for t in ner_extracted)
        reduction = (1 - stats['after_ner_tokens'] / stats['original_tokens']) * 100
        print(f"  ê²°ê³¼: ~{stats['after_ner_tokens']:,} í† í° ({reduction:.1f}% ì ˆê°)")

        # 4. LLMLingua ì••ì¶•
        print(f"\nğŸ“¦ ë‹¨ê³„ 3: LLMLingua ì••ì¶•")
        llmlingua_compressed = []
        for i, text in enumerate(ner_extracted):
            if i % 100 == 0 and i > 0:
                print(f"    ì§„í–‰: {i}/{len(ner_extracted)}")
            compressed = self._compress_with_llmlingua(text, target_ratio=0.2)
            llmlingua_compressed.append(compressed)

        stats['after_llmlingua_tokens'] = sum(len(t.split()) for t in llmlingua_compressed)
        reduction = (1 - stats['after_llmlingua_tokens'] / stats['after_ner_tokens']) * 100
        print(f"  ê²°ê³¼: ~{stats['after_llmlingua_tokens']:,} í† í° ({reduction:.1f}% ì ˆê°)")

        # 5. ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±°
        print(f"\nğŸ§¹ ë‹¨ê³„ 4: ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±°")
        final_data = [self._remove_boilerplate(t) for t in llmlingua_compressed]
        stats['final_tokens'] = sum(len(t.split()) for t in final_data)
        reduction = (1 - stats['final_tokens'] / stats['after_llmlingua_tokens']) * 100
        print(f"  ê²°ê³¼: ~{stats['final_tokens']:,} í† í° ({reduction:.1f}% ì ˆê°)")

        # ìµœì¢… í†µê³„
        print(f"\n{'='*80}")
        print(f"âœ… ì••ì¶• ì™„ë£Œ!")
        print(f"{'='*80}")
        total_reduction = (1 - stats['final_tokens'] / stats['original_tokens']) * 100
        print(f"  ì›ë³¸:     ~{stats['original_tokens']:,} í† í°")
        print(f"  ìµœì¢…:     ~{stats['final_tokens']:,} í† í°")
        print(f"  ì´ ì ˆê°:  {total_reduction:.1f}%")
        print(f"{'='*80}")

        return final_data, stats

    async def evaluate_with_claude_batchprompt(
        self,
        politician_name: str,
        data_list: List[str]
    ) -> Dict:
        """Claude BatchPrompt: 10ê°œ ì¹´í…Œê³ ë¦¬ í•œ ë²ˆì— í‰ê°€"""

        print(f"\n    ğŸ“Š Claude BatchPrompt í‰ê°€")
        print(f"    ë°ì´í„°: {len(data_list)}ê°œ")

        # ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ (ì´ˆì••ì¶• í˜•íƒœ)
        data_json = json.dumps(
            [{"n": i+1, "t": data[:100]} for i, data in enumerate(data_list)],  # ê° 100ìë¡œ ì œí•œ
            ensure_ascii=False
        )

        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=20000,
                system=[
                    {
                        "type": "text",
                        "text": self.minimal_system,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[
                    {
                        "role": "user",
                        "content": f"{politician_name}\n{data_json}"
                    }
                ]
            )

            result = json.loads(response.content[0].text)

            # í† í° ì‚¬ìš©ëŸ‰
            usage = response.usage
            print(f"      í† í°: input={usage.input_tokens}, output={usage.output_tokens}")
            print(f"      ìºì‹œ: creation={getattr(usage, 'cache_creation_input_tokens', 0)}, "
                  f"read={getattr(usage, 'cache_read_input_tokens', 0)}")

            return result

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {str(e)}")
            return {}

    async def evaluate_with_chatgpt_batchprompt(
        self,
        politician_name: str,
        data_list: List[str]
    ) -> Dict:
        """ChatGPT BatchPrompt"""

        print(f"\n    ğŸ“Š ChatGPT BatchPrompt í‰ê°€")

        data_json = json.dumps(
            [{"n": i+1, "t": data[:100]} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": self.minimal_system},
                    {"role": "user", "content": f"{politician_name}\n{data_json}"}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)

            usage = response.usage
            print(f"      í† í°: input={usage.prompt_tokens}, output={usage.completion_tokens}")

            return result

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {str(e)}")
            return {}

    async def evaluate_with_grok_batchprompt(
        self,
        politician_name: str,
        data_list: List[str]
    ) -> Dict:
        """Grok BatchPrompt"""

        print(f"\n    ğŸ“Š Grok BatchPrompt í‰ê°€")

        data_json = json.dumps(
            [{"n": i+1, "t": data[:100]} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.grok_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "grok-beta",
                        "messages": [
                            {"role": "system", "content": self.minimal_system},
                            {"role": "user", "content": f"{politician_name}\n{data_json}"}
                        ],
                        "temperature": 0.3
                    }
                ) as resp:
                    data = await resp.json()
                    result = json.loads(data['choices'][0]['message']['content'])

                    usage = data.get('usage', {})
                    print(f"      í† í°: input={usage.get('prompt_tokens', 0)}, output={usage.get('completion_tokens', 0)}")

                    return result

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {str(e)}")
            return {}

    async def evaluate_one_politician_full_optimized(
        self,
        politician_name: str,
        politician_id: str
    ) -> Dict:
        """1ëª… ì™„ì „ ìµœì í™” í‰ê°€"""

        print(f"\n{'='*80}")
        print(f"ğŸš€ ìµœì²¨ë‹¨ ì´ˆìµœì í™” í‰ê°€ ì‹œì‘")
        print(f"{'='*80}")
        print(f"ì •ì¹˜ì¸: {politician_name}")
        print(f"ëª©í‘œ: 98% í† í° ì ˆê°")
        print(f"{'='*80}")

        start_time = datetime.now()

        # 1. ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“¥ ë‹¨ê³„ 1: DB ë¡œë“œ")
        raw_data = self._load_data_from_db(politician_id)
        print(f"  ë¡œë“œ: {len(raw_data)}ê°œ")

        # 2. ì´ˆì••ì¶•
        compressed_data, compression_stats = self._ultra_compress_data_list(raw_data)

        # 3. 3ê°œ AI ë³‘ë ¬ í‰ê°€ (BatchPrompt)
        print(f"\nğŸ“Š ë‹¨ê³„ 2: 3ê°œ AI ë³‘ë ¬ í‰ê°€ (BatchPrompt)")

        chatgpt_task = self.evaluate_with_chatgpt_batchprompt(politician_name, compressed_data)
        grok_task = self.evaluate_with_grok_batchprompt(politician_name, compressed_data)
        claude_task = self.evaluate_with_claude_batchprompt(politician_name, compressed_data)

        chatgpt_result, grok_result, claude_result = await asyncio.gather(
            chatgpt_task, grok_task, claude_task
        )

        # 4. Pooling ê³„ì‚°
        print(f"\nğŸ“ˆ ë‹¨ê³„ 3: Pooling ê³„ì‚°")
        pooling_scores = self._calculate_pooling_from_batch(
            chatgpt_result, grok_result, claude_result
        )

        elapsed = (datetime.now() - start_time).total_seconds()

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*80}")
        print(f"âœ… í‰ê°€ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        print(f"  ë°ì´í„°: {compression_stats['original_count']}ê°œ")
        print(f"  í† í° ì ˆê°: {(1 - compression_stats['final_tokens'] / compression_stats['original_tokens']) * 100:.1f}%")
        print(f"{'='*80}")

        print(f"\nğŸ“Š Pooling ì ìˆ˜:")
        for cat in CATEGORIES:
            score = pooling_scores.get(cat, 0)
            print(f"  {CATEGORY_KOREAN[cat]:8s}: {score:.2f}ì ")

        avg_score = sum(pooling_scores.values()) / len(pooling_scores) if pooling_scores else 0
        print(f"\n  í‰ê· : {avg_score:.2f}ì ")

        return {
            'politician': politician_name,
            'politician_id': politician_id,
            'compression_stats': compression_stats,
            'chatgpt': chatgpt_result,
            'grok': grok_result,
            'claude': claude_result,
            'pooling': pooling_scores,
            'elapsed_seconds': elapsed
        }

    def _calculate_pooling_from_batch(
        self,
        chatgpt: Dict,
        grok: Dict,
        claude: Dict
    ) -> Dict:
        """BatchPrompt ê²°ê³¼ì—ì„œ Pooling ê³„ì‚°"""

        pooling = {}

        for category in CATEGORIES:
            chatgpt_scores = chatgpt.get(category, [])
            grok_scores = grok.get(category, [])
            claude_scores = claude.get(category, [])

            if chatgpt_scores and grok_scores and claude_scores:
                chatgpt_avg = sum(chatgpt_scores) / len(chatgpt_scores)
                grok_avg = sum(grok_scores) / len(grok_scores)
                claude_avg = sum(claude_scores) / len(claude_scores)

                pooling[category] = (chatgpt_avg + grok_avg + claude_avg) / 3
            else:
                pooling[category] = 0

        return pooling


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    print("\n" + "="*80)
    print("ğŸš€ ìµœì²¨ë‹¨ ì´ˆìµœì í™” Pooling í‰ê°€ ì‹œìŠ¤í…œ")
    print("="*80)
    print(f"í…ŒìŠ¤íŠ¸: {TEST_POLITICIAN['name']}")
    print(f"ëª©í‘œ: 98% í† í° ì ˆê°")
    print(f"")
    print(f"ìµœì í™” ê¸°ë²•:")
    print(f"  1. ì¤‘ë³µ ì œê±°")
    print(f"  2. NER í•µì‹¬ ì¶”ì¶œ")
    print(f"  3. LLMLingua ì••ì¶• (20ë°°)")
    print(f"  4. ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±°")
    print(f"  5. System Message ìµœì†Œí™” (15í† í°)")
    print(f"  6. Prompt Caching")
    print(f"  7. BatchPrompt (1íšŒ API)")
    print("="*80)

    evaluator = FullOptimizedEvaluator()

    # í‰ê°€ ì‹¤í–‰
    result = await evaluator.evaluate_one_politician_full_optimized(
        TEST_POLITICIAN['name'],
        TEST_POLITICIAN['id']
    )

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"FULL_optimized_test_{TEST_POLITICIAN['name']}_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")


if __name__ == "__main__":
    asyncio.run(main())
