#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
95% í† í° ì ˆê° í…ŒìŠ¤íŠ¸ - Prompt Caching + Category Integration
- Anthropic Prompt Caching (4 cache breakpoints, 1-hour TTL)
- 10ê°œ ì¹´í…Œê³ ë¦¬ í†µí•© í‰ê°€ (1íšŒ API í˜¸ì¶œ)
- ì „ì²´ 150ê°œ ë°ì´í„° ì‚¬ìš© (ìƒ˜í”Œë§ ì—†ìŒ)
- ëª©í‘œ: 95% ì´ìƒ í† í° ì ˆê°
"""

import asyncio
import aiohttp
import json
import os
import re
import hashlib
from datetime import datetime
from typing import List, Dict
import anthropic
import openai
from supabase import create_client
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROK_API_KEY = os.getenv("XAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# í‰ê°€ ì¹´í…Œê³ ë¦¬
CATEGORIES = [
    "Expertise", "Leadership", "Vision", "Integrity", "Ethics",
    "Accountability", "Transparency", "Communication", "Responsiveness", "PublicInterest"
]

CATEGORY_KOREAN = {
    "Expertise": "ì „ë¬¸ì„±",
    "Leadership": "ë¦¬ë”ì‹­",
    "Vision": "ë¹„ì „",
    "Integrity": "ì²­ë ´ë„",
    "Ethics": "ìœ¤ë¦¬ì„±",
    "Accountability": "ì±…ì„ê°",
    "Transparency": "íˆ¬ëª…ì„±",
    "Communication": "ì†Œí†µëŠ¥ë ¥",
    "Responsiveness": "ëŒ€ì‘ì„±",
    "PublicInterest": "ê³µìµì„±"
}

# í…ŒìŠ¤íŠ¸ ì •ì¹˜ì¸ (ì†¡ì„ì¤€)
TEST_POLITICIAN = {
    "id": "023139c6",
    "name": "ì†¡ì„ì¤€"
}


class TokenSavingsEvaluator:
    """95% í† í° ì ˆê° í‰ê°€ ì—”ì§„"""

    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        self.grok_api_key = GROK_API_KEY
        self.supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.token_usage = {
            'claude': {'input': 0, 'output': 0, 'cache_creation': 0, 'cache_read': 0},
            'chatgpt': {'input': 0, 'output': 0},
            'grok': {'input': 0, 'output': 0}
        }

    def _get_evaluation_criteria(self) -> str:
        """í‰ê°€ ê¸°ì¤€ (Cache Breakpoint 1)"""
        return """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ 10ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ì •ì¹˜ì¸ì„ í‰ê°€í•˜ì„¸ìš”:

1. Expertise (ì „ë¬¸ì„±): ì •ì±… ì „ë¬¸ì„±, ë¶„ì•¼ ì§€ì‹
2. Leadership (ë¦¬ë”ì‹­): ì¡°ì§ ê´€ë¦¬, ê²°ë‹¨ë ¥
3. Vision (ë¹„ì „): ë¯¸ë˜ ë¹„ì „, ì •ì±… ë°©í–¥ì„±
4. Integrity (ì²­ë ´ë„): ë¶€íŒ¨ ì—†ìŒ, ë„ë•ì„±
5. Ethics (ìœ¤ë¦¬ì„±): ìœ¤ë¦¬ì  íŒë‹¨, ê°€ì¹˜ê´€
6. Accountability (ì±…ì„ê°): ì•½ì† ì´í–‰, ì±…ì„ ì˜ì‹
7. Transparency (íˆ¬ëª…ì„±): ì •ë³´ ê³µê°œ, íˆ¬ëª…í•œ ì˜ì‚¬ê²°ì •
8. Communication (ì†Œí†µëŠ¥ë ¥): êµ­ë¯¼ê³¼ì˜ ì†Œí†µ, ì„¤ëª… ëŠ¥ë ¥
9. Responsiveness (ëŒ€ì‘ì„±): ë¯¼ì› ëŒ€ì‘, ìœ„ê¸° ê´€ë¦¬
10. PublicInterest (ê³µìµì„±): ê³µê³µì˜ ì´ìµ ìš°ì„ 

í‰ê°€ ë°©ë²•:
- ê° ë°ì´í„°ì—ì„œ ì •ì¹˜ì¸ì˜ í–‰ë™/ë°œì–¸ì„ ë¶„ì„
- ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ 0-100ì  ì ìˆ˜ ë¶€ì—¬
- ê°ê´€ì  ì‚¬ì‹¤ ê¸°ë°˜ í‰ê°€
- ê¸ì •ì /ë¶€ì •ì  ìš”ì†Œ ëª¨ë‘ ê³ ë ¤"""

    def _get_output_format(self) -> str:
        """ì¶œë ¥ í˜•ì‹ (Cache Breakpoint 4)"""
        return """
ì¶œë ¥ í˜•ì‹ (JSON):
{
  "Expertise": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Leadership": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Vision": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Integrity": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Ethics": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Accountability": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Transparency": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Communication": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "Responsiveness": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150],
  "PublicInterest": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜150]
}

ì£¼ì˜: ê° ì¹´í…Œê³ ë¦¬ëŠ” ì •í™•íˆ 150ê°œì˜ ì ìˆ˜ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
"""

    def _load_data_from_db(self, politician_id: str) -> List[Dict]:
        """DBì—ì„œ ë°ì´í„° ë¡œë“œ"""
        response = self.supabase.table('collected_data').select(
            'data_title, data_content, ai_name, category_name, source_type'
        ).eq('politician_id', politician_id).execute()

        return response.data

    def _compress_data(self, data: str) -> str:
        """ë°ì´í„° ì••ì¶• (ë©”íƒ€ë°ì´í„° ì œê±°, í•µì‹¬ë§Œ ì¶”ì¶œ)"""
        # 1. ë©”íƒ€ë°ì´í„° ì œê±°
        data = re.sub(r'\[.*?\]', '', data)

        # 2. ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        remove_phrases = [
            'ê´€ë ¨ ê¸°ì‚¬', 'ë” ë³´ê¸°', 'ê¸°ì‚¬ ì „ë¬¸', 'ì¶œì²˜:', 'ì €ì‘ê¶Œ',
            'ë¬´ë‹¨ ì „ì¬', 'ì¬ë°°í¬ ê¸ˆì§€', 'ë‰´ìŠ¤', 'ë³´ë„ìë£Œ',
            'ë”ë³´ê¸°', 'â–¶', 'â—†', 'â€»', 'â– '
        ]

        for phrase in remove_phrases:
            data = data.replace(phrase, '')

        # 3. ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°
        data = re.sub(r'\s+', ' ', data)
        data = data.strip()

        # 4. í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ (ì²« 3ë¬¸ì¥ + í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥)
        sentences = [s.strip() for s in data.split('.') if s.strip()]

        keywords = ['ì •ì±…', 'ë²•ì•ˆ', 'ì˜ˆì‚°', 'ê³µì•½', 'ê°œí˜', 'ì¶”ì§„', 'ë°œí‘œ', 'ì œì•ˆ', 'ê³„íš']

        key_sentences = sentences[:3]

        for sent in sentences[3:]:
            if any(kw in sent for kw in keywords):
                key_sentences.append(sent)
                if len(key_sentences) >= 5:
                    break

        compressed = '. '.join(key_sentences[:5])
        return compressed

    def _remove_duplicates(self, data_list: List[str]) -> List[str]:
        """ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for data in data_list:
            content_hash = hashlib.md5(data.encode('utf-8')).hexdigest()[:16]
            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(data)

        return unique

    def _prepare_data_list(self, raw_data: List[Dict]) -> List[str]:
        """ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ (150ê°œë¡œ ì œí•œ)"""
        print(f"\n  ğŸ“¦ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        print(f"  ì›ë³¸: {len(raw_data)}ê°œ í•­ëª©")

        # 1. í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text_data = []
        for item in raw_data:
            title = item.get('data_title', '')
            content = item.get('data_content', '')
            text = f"{title}. {content}"
            text_data.append(text)

        # 2. ì¤‘ë³µ ì œê±°
        unique_data = self._remove_duplicates(text_data)
        print(f"  ì¤‘ë³µ ì œê±° í›„: {len(unique_data)}ê°œ í•­ëª©")

        # 3. 150ê°œë¡œ ì œí•œ
        limited_data = unique_data[:150]
        print(f"  150ê°œë¡œ ì œí•œ: {len(limited_data)}ê°œ í•­ëª©")

        # 4. ì••ì¶•
        compressed_data = [self._compress_data(d) for d in limited_data]
        print(f"  ì••ì¶• ì™„ë£Œ")

        return compressed_data

    async def evaluate_with_claude_cached(
        self,
        politician_name: str,
        data_list: List[str]
    ) -> Dict:
        """Claude Prompt Caching í‰ê°€ (95% í† í° ì ˆê°)"""

        print(f"\n  ğŸ“Š Claude í‰ê°€ ì‹œì‘ (Prompt Caching)")

        # ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
        data_json = json.dumps(
            [{"index": i+1, "content": data} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        # 4ê°œ Cache Breakpoints ì„¤ì •
        system_messages = [
            {
                "type": "text",
                "text": self._get_evaluation_criteria(),
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": f"ì •ì¹˜ì¸: {politician_name}",
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": data_json,
                "cache_control": {"type": "ephemeral"}
            },
            {
                "type": "text",
                "text": self._get_output_format(),
                "cache_control": {"type": "ephemeral"}
            }
        ]

        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=16000,
                system=system_messages,
                messages=[
                    {
                        "role": "user",
                        "content": f"ìœ„ {len(data_list)}ê°œ ë°ì´í„°ë¥¼ ëª¨ë‘ ë¶„ì„í•˜ê³ , 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê° ë°ì´í„°ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”."
                    }
                ]
            )

            result = json.loads(response.content[0].text)

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            usage = response.usage
            self.token_usage['claude']['input'] += usage.input_tokens
            self.token_usage['claude']['output'] += usage.output_tokens
            self.token_usage['claude']['cache_creation'] += getattr(usage, 'cache_creation_input_tokens', 0)
            self.token_usage['claude']['cache_read'] += getattr(usage, 'cache_read_input_tokens', 0)

            print(f"    âœ… Claude ì™„ë£Œ")
            print(f"    í† í°: input={usage.input_tokens}, output={usage.output_tokens}")
            print(f"    ìºì‹œ: creation={getattr(usage, 'cache_creation_input_tokens', 0)}, read={getattr(usage, 'cache_read_input_tokens', 0)}")

            return result

        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {str(e)}")
            return None

    async def evaluate_with_chatgpt(
        self,
        politician_name: str,
        data_list: List[str]
    ) -> Dict:
        """ChatGPT í‰ê°€"""

        print(f"\n  ğŸ“Š ChatGPT í‰ê°€ ì‹œì‘")

        data_json = json.dumps(
            [{"index": i+1, "content": data} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        system_msg = self._get_evaluation_criteria() + self._get_output_format()

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": f"ì •ì¹˜ì¸: {politician_name}\n\në°ì´í„°:\n{data_json}\n\nìœ„ {len(data_list)}ê°œ ë°ì´í„°ë¥¼ ëª¨ë‘ ë¶„ì„í•˜ê³ , 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê° ë°ì´í„°ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”."}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            usage = response.usage
            self.token_usage['chatgpt']['input'] += usage.prompt_tokens
            self.token_usage['chatgpt']['output'] += usage.completion_tokens

            print(f"    âœ… ChatGPT ì™„ë£Œ")
            print(f"    í† í°: input={usage.prompt_tokens}, output={usage.completion_tokens}")

            return result

        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {str(e)}")
            return None

    async def evaluate_with_grok(
        self,
        politician_name: str,
        data_list: List[str]
    ) -> Dict:
        """Grok í‰ê°€"""

        print(f"\n  ğŸ“Š Grok í‰ê°€ ì‹œì‘")

        data_json = json.dumps(
            [{"index": i+1, "content": data} for i, data in enumerate(data_list)],
            ensure_ascii=False
        )

        system_msg = self._get_evaluation_criteria() + self._get_output_format()

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.grok_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "grok-beta",
                        "messages": [
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": f"ì •ì¹˜ì¸: {politician_name}\n\në°ì´í„°:\n{data_json}\n\nìœ„ {len(data_list)}ê°œ ë°ì´í„°ë¥¼ ëª¨ë‘ ë¶„ì„í•˜ê³ , 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê° ë°ì´í„°ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”."}
                        ],
                        "temperature": 0.3
                    }
                ) as resp:
                    data = await resp.json()
                    result = json.loads(data['choices'][0]['message']['content'])

                    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                    usage = data.get('usage', {})
                    self.token_usage['grok']['input'] += usage.get('prompt_tokens', 0)
                    self.token_usage['grok']['output'] += usage.get('completion_tokens', 0)

                    print(f"    âœ… Grok ì™„ë£Œ")
                    print(f"    í† í°: input={usage.get('prompt_tokens', 0)}, output={usage.get('completion_tokens', 0)}")

                    return result

        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {str(e)}")
            return None

    async def test_95percent_savings(self, politician_id: str) -> Dict:
        """95% í† í° ì ˆê° í…ŒìŠ¤íŠ¸"""

        print(f"\n{'='*80}")
        print(f"ğŸš€ 95% í† í° ì ˆê° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'='*80}")
        print(f"ì •ì¹˜ì¸ ID: {politician_id}")
        print(f"ë°©ë²•: Prompt Caching (4 breakpoints) + Category Integration")
        print(f"{'='*80}")

        start_time = datetime.now()

        # 1. ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“¥ 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")
        raw_data = self._load_data_from_db(politician_id)
        print(f"  DBì—ì„œ {len(raw_data)}ê°œ í•­ëª© ë¡œë“œ")

        # 2. ë°ì´í„° ì¤€ë¹„
        print(f"\nğŸ—œï¸ 2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„")
        prepared_data = self._prepare_data_list(raw_data)
        print(f"  ìµœì¢…: {len(prepared_data)}ê°œ ë°ì´í„°")

        # ì •ì¹˜ì¸ ì´ë¦„ ì¡°íšŒ
        politician_response = self.supabase.table('politicians').select('name').eq('id', politician_id).execute()
        politician_name = politician_response.data[0]['name'] if politician_response.data else "Unknown"

        # 3. 3ê°œ AI ë³‘ë ¬ í‰ê°€
        print(f"\nğŸ“Š 3ë‹¨ê³„: 3ê°œ AI ë³‘ë ¬ í‰ê°€")

        chatgpt_task = self.evaluate_with_chatgpt(politician_name, prepared_data)
        grok_task = self.evaluate_with_grok(politician_name, prepared_data)
        claude_task = self.evaluate_with_claude_cached(politician_name, prepared_data)

        chatgpt_result, grok_result, claude_result = await asyncio.gather(
            chatgpt_task, grok_task, claude_task
        )

        # 4. Pooling ì ìˆ˜ ê³„ì‚°
        print(f"\nğŸ“ˆ 4ë‹¨ê³„: Pooling ì ìˆ˜ ê³„ì‚°")
        pooling_scores = self._calculate_pooling(chatgpt_result, grok_result, claude_result)

        elapsed = (datetime.now() - start_time).total_seconds()

        # 5. í† í° ì‚¬ìš©ëŸ‰ ë³´ê³ 
        print(f"\n{'='*80}")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        print(f"ì •ì¹˜ì¸: {politician_name}")
        print(f"ë°ì´í„° ìˆ˜: {len(prepared_data)}ê°œ")
        print(f"\nğŸ“Š í† í° ì‚¬ìš©ëŸ‰:")
        print(f"  Claude:")
        print(f"    Input: {self.token_usage['claude']['input']:,}")
        print(f"    Output: {self.token_usage['claude']['output']:,}")
        print(f"    Cache Creation: {self.token_usage['claude']['cache_creation']:,}")
        print(f"    Cache Read: {self.token_usage['claude']['cache_read']:,}")
        print(f"  ChatGPT:")
        print(f"    Input: {self.token_usage['chatgpt']['input']:,}")
        print(f"    Output: {self.token_usage['chatgpt']['output']:,}")
        print(f"  Grok:")
        print(f"    Input: {self.token_usage['grok']['input']:,}")
        print(f"    Output: {self.token_usage['grok']['output']:,}")

        total_input = (self.token_usage['claude']['input'] +
                       self.token_usage['chatgpt']['input'] +
                       self.token_usage['grok']['input'])
        total_output = (self.token_usage['claude']['output'] +
                        self.token_usage['chatgpt']['output'] +
                        self.token_usage['grok']['output'])
        total_cache = (self.token_usage['claude']['cache_creation'] +
                       self.token_usage['claude']['cache_read'])

        print(f"\n  ì´í•©:")
        print(f"    Total Input: {total_input:,}")
        print(f"    Total Output: {total_output:,}")
        print(f"    Total Cache: {total_cache:,}")
        print(f"    Grand Total: {total_input + total_output + total_cache:,}")

        print(f"\nğŸ“Š Pooling ì ìˆ˜:")
        for cat in CATEGORIES:
            score = pooling_scores.get(cat, 0)
            print(f"  {CATEGORY_KOREAN[cat]:8s}: {score:.2f}ì ")

        avg_score = sum(pooling_scores.values()) / len(pooling_scores)
        print(f"\n  í‰ê· : {avg_score:.2f}ì ")
        print(f"{'='*80}")

        return {
            'politician_id': politician_id,
            'politician_name': politician_name,
            'data_count': len(prepared_data),
            'chatgpt': chatgpt_result,
            'grok': grok_result,
            'claude': claude_result,
            'pooling': pooling_scores,
            'token_usage': self.token_usage,
            'elapsed_seconds': elapsed
        }

    def _calculate_pooling(self, chatgpt: Dict, grok: Dict, claude: Dict) -> Dict:
        """Pooling ì ìˆ˜ ê³„ì‚°"""
        if not all([chatgpt, grok, claude]):
            return {}

        pooling = {}

        for category in CATEGORIES:
            chatgpt_scores = chatgpt.get(category, [])
            grok_scores = grok.get(category, [])
            claude_scores = claude.get(category, [])

            if chatgpt_scores and grok_scores and claude_scores:
                chatgpt_avg = sum(chatgpt_scores) / len(chatgpt_scores)
                grok_avg = sum(grok_scores) / len(grok_scores)
                claude_avg = sum(claude_scores) / len(claude_scores)

                pooling[category] = (chatgpt_avg + grok_avg + claude_avg) / 3
            else:
                pooling[category] = 0

        return pooling


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    print("\n" + "="*80)
    print("ğŸš€ 95% í† í° ì ˆê° í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {TEST_POLITICIAN['id']}")
    print(f"ë°©ë²•: Prompt Caching + Category Integration")
    print(f"ëª©í‘œ: 95% í† í° ì ˆê°")
    print("="*80)

    evaluator = TokenSavingsEvaluator()

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = await evaluator.test_95percent_savings(TEST_POLITICIAN['id'])

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"95percent_savings_test_{TEST_POLITICIAN['id']}_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")


if __name__ == "__main__":
    asyncio.run(main())
