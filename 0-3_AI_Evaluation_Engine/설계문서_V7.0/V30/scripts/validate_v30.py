# -*- coding: utf-8 -*-
"""
V30 ê²€ì¦, ì¤‘ë³µ ì œê±° ë° ì¬ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

í•µì‹¬ (V30 ê²€ì¦):
1. URL ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (HEAD/GET ìš”ì²­)
2. source_type ê·œì¹™ ê²€ì¦ (OFFICIAL/PUBLIC ë„ë©”ì¸ ë§¤ì¹­)
3. í•„ìˆ˜ í•„ë“œ ê²€ì¦ (title, content, source_url)
4. ê¸°ê°„ ì œí•œ ê²€ì¦ (OFFICIAL 4ë…„, PUBLIC 2ë…„)
5. ì¤‘ë³µ ë°ì´í„° ìë™ ì œê±° (ê°™ì€ AI + ê°™ì€ URL) âœ… í†µí•©
6. ê²€ì¦ ì‹¤íŒ¨ í•­ëª© ìë™ ì¬ìˆ˜ì§‘

í”„ë¡œì„¸ìŠ¤:
[1] ê²€ì¦ (validate): ìˆ˜ì§‘ëœ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    - ì¤‘ë³µ ë°œê²¬ ì‹œ ìë™ ì‚­ì œ (evaluations í¬í•¨)
[2] ì¬ìˆ˜ì§‘ (recollect): ê²€ì¦ ì‹¤íŒ¨ë¶„ í•´ë‹¹ AIë¡œ ì¬ìˆ˜ì§‘
[3] ì¬ê²€ì¦: ì¬ìˆ˜ì§‘ ë°ì´í„° ë‹¤ì‹œ ê²€ì¦

ì‚¬ìš©ë²•:
    # ì „ì²´ ê²€ì¦ + ì¬ìˆ˜ì§‘
    python validate_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --mode=all

    # ê²€ì¦ë§Œ
    python validate_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --mode=validate

    # ì¬ìˆ˜ì§‘ë§Œ
    python validate_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --mode=recollect

    # íŠ¹ì • AIë§Œ
    python validate_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --ai=Perplexity
"""

import os
import sys
import json
import re
import argparse
import time
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from supabase import create_client
from dotenv import load_dotenv

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except AttributeError:
        # ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆê±°ë‚˜ bufferê°€ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
        pass

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

# V30 í…Œì´ë¸”ëª…
TABLE_COLLECTED_DATA = "collected_data_v30"
TABLE_EVALUATIONS = "evaluations_v30"

# ê³µì‹ ë°ì´í„° ë„ë©”ì¸ (OFFICIALë¡œ ì¸ì •ë˜ëŠ” ë„ë©”ì¸)
OFFICIAL_DOMAINS = [
    "assembly.go.kr",
    "likms.assembly.go.kr",
    "mois.go.kr",
    "korea.kr",
    "nec.go.kr",
    "bai.go.kr",
    "pec.go.kr",
    "scourt.go.kr",
    "nesdc.go.kr",
    "manifesto.or.kr",
    "peoplepower21.org",
    "theminjoo.kr",
    "seoul.go.kr",
    "gg.go.kr",
    "busan.go.kr",
    "incheon.go.kr",
    "daegu.go.kr",
    "daejeon.go.kr",
    "gwangju.go.kr",
    "ulsan.go.kr",
    "sejong.go.kr",
    "open.go.kr",
    "acrc.go.kr",
    "humanrights.go.kr"
]

# SNS ë„ë©”ì¸ (URL ê²€ì¦ ì œì™¸)
SNS_DOMAINS = [
    "twitter.com",
    "x.com",
    "facebook.com",
    "instagram.com",
    "youtube.com",
    "youtu.be",
    "tiktok.com"
]

# ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = [
    ("expertise", "ì „ë¬¸ì„±"),
    ("leadership", "ë¦¬ë”ì‹­"),
    ("vision", "ë¹„ì „"),
    ("integrity", "ì²­ë ´ì„±"),
    ("ethics", "ìœ¤ë¦¬ì„±"),
    ("consistency", "ì¼ê´€ì„±"),
    ("crisis", "ìœ„ê¸°ëŒ€ì‘"),
    ("communication", "ì†Œí†µëŠ¥ë ¥"),
    ("responsiveness", "ëŒ€ì‘ì„±"),
    ("publicinterest", "ê³µìµì„±")
]

# ê²€ì¦ ê²°ê³¼ ì½”ë“œ
VALIDATION_CODES = {
    "VALID": "ìœ íš¨",
    "INVALID_URL": "URL ì ‘ì† ë¶ˆê°€",
    "EMPTY_URL": "URL ë¹„ì–´ìˆìŒ",
    "FAKE_URL": "ê°€ì§œ URL íŒ¨í„´",
    "WRONG_SOURCE_TYPE": "source_type ë¶ˆì¼ì¹˜",
    "MISSING_FIELD": "í•„ìˆ˜ í•„ë“œ ëˆ„ë½",
    "DATE_OUT_OF_RANGE": "ê¸°ê°„ ì´ˆê³¼",
    "DUPLICATE": "ì¤‘ë³µ ë°ì´í„°"
}


def get_date_range():
    """V30 ê¸°ê°„ ì œí•œ ê³„ì‚°"""
    evaluation_date = datetime.now()
    official_start = evaluation_date - timedelta(days=365*4)  # 4ë…„
    public_start = evaluation_date - timedelta(days=365*2)    # 2ë…„

    return {
        'official_start': official_start,
        'official_end': evaluation_date,
        'public_start': public_start,
        'public_end': evaluation_date,
    }


def is_sns_url(url):
    """SNS URL ì—¬ë¶€ í™•ì¸"""
    if not url:
        return False
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        return any(sns in domain for sns in SNS_DOMAINS)
    except:
        return False


def is_official_domain(url):
    """ê³µì‹ ë„ë©”ì¸ ì—¬ë¶€ í™•ì¸"""
    if not url:
        return False
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        return any(official in domain for official in OFFICIAL_DOMAINS)
    except:
        return False


def is_fake_url_pattern(url):
    """ê°€ì§œ URL íŒ¨í„´ ê°ì§€"""
    if not url:
        return True

    fake_patterns = [
        r'/\d{10,}',           # ìˆ«ìë§Œ ìˆëŠ” ê¸´ ID
        r'ncd=\d{7}$',         # KBS ê°€ì§œ íŒ¨í„´
        r'/article/\d{4}/\d{2}/\d{2}/[a-z\-]+$',  # ë‚ ì§œ+slug ê°€ì§œ íŒ¨í„´
        r'example\.com',
        r'test\.com',
        r'localhost',
        r'0{5,}',              # 00000 íŒ¨í„´
    ]

    for pattern in fake_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return True

    return False


def check_url_exists(url, timeout=10):
    """URL ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    if not url or url.strip() == '':
        return False, "EMPTY_URL"

    # SNSëŠ” ê²€ì¦ ì œì™¸ (ì ‘ê·¼ ì œí•œ ìˆìŒ)
    if is_sns_url(url):
        return True, "VALID"

    # ê°€ì§œ URL íŒ¨í„´ ì²´í¬
    if is_fake_url_pattern(url):
        return False, "FAKE_URL"

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # HEAD ìš”ì²­ ë¨¼ì € ì‹œë„
        try:
            response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)
            if response.status_code < 400:
                return True, "VALID"
        except:
            pass

        # HEAD ì‹¤íŒ¨ ì‹œ GET ì‹œë„
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        if response.status_code < 400:
            return True, "VALID"
        else:
            return False, "INVALID_URL"

    except requests.exceptions.Timeout:
        return False, "INVALID_URL"
    except requests.exceptions.ConnectionError:
        return False, "INVALID_URL"
    except Exception as e:
        return False, "INVALID_URL"


def validate_source_type(item):
    """source_type ê·œì¹™ ê²€ì¦"""
    url = item.get('source_url', '')
    declared_type = item.get('data_type', '').upper()

    if not url:
        return True, "VALID"  # URL ì—†ìœ¼ë©´ ì´ ê²€ì¦ì€ íŒ¨ìŠ¤

    is_official = is_official_domain(url)

    if declared_type == 'OFFICIAL' and not is_official:
        return False, "WRONG_SOURCE_TYPE"

    if declared_type == 'PUBLIC' and is_official:
        # PUBLICì¸ë° ê³µì‹ ë„ë©”ì¸ì´ë©´ ê²½ê³  (ì—ëŸ¬ëŠ” ì•„ë‹˜)
        return True, "VALID"

    return True, "VALID"


def validate_required_fields(item):
    """í•„ìˆ˜ í•„ë“œ ê²€ì¦"""
    required = ['title', 'content', 'source_url']

    for field in required:
        value = item.get(field, '')
        if not value or str(value).strip() == '':
            # source_urlì´ ë¹„ì–´ìˆì–´ë„ ì˜ˆì™¸ í—ˆìš©
            if field == 'source_url':
                source_name = item.get('source_name', '')
                collector_ai = item.get('collector_ai', '')
                data_type = item.get('data_type', '')

                # Grok: X/íŠ¸ìœ„í„°ëŠ” "X/@ê³„ì •ëª…" í—ˆìš©
                if 'X/' in source_name or 'twitter' in source_name.lower():
                    continue

                # Gemini PUBLIC: ì„ì‹œ ì½˜í…ì¸ ëŠ” "Platform/@ê³„ì •ëª…" í—ˆìš©
                # (Instagram Stories, Facebook Live, YouTube Live ë“±)
                if collector_ai == 'Gemini' and data_type == 'public':
                    ephemeral_platforms = ['Instagram/', 'Facebook/', 'Live/', 'Story/']
                    if any(platform in source_name for platform in ephemeral_platforms):
                        continue

            return False, "MISSING_FIELD"

    return True, "VALID"


def validate_date_range(item):
    """ê¸°ê°„ ì œí•œ ê²€ì¦"""
    date_range = get_date_range()
    data_type = item.get('data_type', 'public').lower()
    pub_date_str = item.get('published_date')

    if not pub_date_str:
        return True, "VALID"  # ë‚ ì§œ ì—†ìœ¼ë©´ íŒ¨ìŠ¤

    try:
        if isinstance(pub_date_str, str):
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
        else:
            pub_date = pub_date_str

        if data_type == 'official':
            if pub_date < date_range['official_start']:
                return False, "DATE_OUT_OF_RANGE"
        else:
            if pub_date < date_range['public_start']:
                return False, "DATE_OUT_OF_RANGE"

        return True, "VALID"

    except:
        return True, "VALID"  # íŒŒì‹± ì‹¤íŒ¨ë©´ íŒ¨ìŠ¤


def check_duplicate(item):
    """ì¤‘ë³µ ë°ì´í„° ê²€ì¦

    ê·œì¹™: ê°™ì€ AIê°€ ê°™ì€ URLì„ ì´ë¯¸ ìˆ˜ì§‘í–ˆëŠ”ì§€ í™•ì¸
    - ê°™ì€ politician_id + ê°™ì€ collector_ai + ê°™ì€ source_url = ì¤‘ë³µ
    - ë‹¤ë¥¸ AIê°€ ê°™ì€ URL ìˆ˜ì§‘ = ì¤‘ë³µ ì•„ë‹˜ (ìì—° ê°€ì¤‘ì¹˜)
    """
    politician_id = item.get('politician_id')
    collector_ai = item.get('collector_ai')
    source_url = item.get('source_url', '')
    item_id = item.get('id')  # ìê¸° ìì‹  ì œì™¸ìš©

    # URL ì •ê·œí™” (ê³µë°± ì œê±°)
    source_url_normalized = str(source_url).strip() if source_url else ''

    if not source_url_normalized:  # URL ì—†ìœ¼ë©´ ì¤‘ë³µ ì²´í¬ ë¶ˆê°€ëŠ¥
        return True, "VALID"

    try:
        # ê°™ì€ ì •ì¹˜ì¸, ê°™ì€ AI, ê°™ì€ URLì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        query = supabase.table(TABLE_COLLECTED_DATA)\
            .select('id', count='exact')\
            .eq('politician_id', politician_id)\
            .eq('collector_ai', collector_ai)\
            .eq('source_url', source_url_normalized)

        # ìê¸° ìì‹  ì œì™¸
        if item_id:
            query = query.neq('id', item_id)

        existing = query.execute()

        if existing.count and existing.count > 0:
            return False, "DUPLICATE"

        return True, "VALID"
    except Exception as e:
        print(f"  ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
        return True, "VALID"  # ì˜¤ë¥˜ ì‹œ íŒ¨ìŠ¤


def validate_item(item):
    """ë‹¨ì¼ í•­ëª© ì¢…í•© ê²€ì¦"""
    results = []

    # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
    valid, code = validate_required_fields(item)
    if not valid:
        return False, code

    # 2. URL ì¡´ì¬ ê²€ì¦
    url = item.get('source_url', '')
    collector_ai = item.get('collector_ai', '').lower()

    # V30 ê·œì •: Grokì˜ X/íŠ¸ìœ„í„° ë°ì´í„°ëŠ” URL í•„ìš” ì—†ìŒ
    if collector_ai == 'grok':
        # X/íŠ¸ìœ„í„°ëŠ” URL ì—†ìŒ ë˜ëŠ” "X/@ê³„ì •ëª…" í˜•ì‹ í—ˆìš©
        if not url or url.startswith('X/@') or url.startswith('x.com') or url.startswith('twitter.com'):
            pass  # ìœ íš¨ - URL ê²€ì¦ ê±´ë„ˆë›°ê¸°
        elif url and not is_sns_url(url):
            valid, code = check_url_exists(url)
            if not valid:
                return False, code
    else:
        # Gemini, PerplexityëŠ” ì •ìƒ URL ê²€ì¦
        if url and not is_sns_url(url):
            valid, code = check_url_exists(url)
            if not valid:
                return False, code

    # 3. source_type ê²€ì¦
    valid, code = validate_source_type(item)
    if not valid:
        return False, code

    # 4. ê¸°ê°„ ê²€ì¦
    valid, code = validate_date_range(item)
    if not valid:
        return False, code

    # 5. ì¤‘ë³µ ê²€ì¦
    valid, code = check_duplicate(item)
    if not valid:
        return False, code

    return True, "VALID"


def get_collected_data(politician_id, ai_name=None, category=None):
    """ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ"""
    try:
        query = supabase.table(TABLE_COLLECTED_DATA)\
            .select('*')\
            .eq('politician_id', politician_id)

        if ai_name:
            query = query.eq('collector_ai', ai_name.lower())

        if category:
            query = query.eq('category', category.lower())

        result = query.execute()
        return result.data if result.data else []

    except Exception as e:
        print(f"  âš ï¸ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def validate_collected_data(politician_id, politician_name, ai_name=None, category=None):
    """ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦"""
    print(f"\n{'='*60}")
    print(f"[ê²€ì¦] {politician_name} ({politician_id})")
    print(f"{'='*60}")

    items = get_collected_data(politician_id, ai_name, category)
    print(f"ì´ {len(items)}ê°œ í•­ëª© ê²€ì¦ ì‹œì‘...")

    valid_count = 0
    invalid_items = []
    duplicate_removed = 0  # ì¤‘ë³µ ì œê±° ì¹´ìš´íŠ¸

    for i, item in enumerate(items):
        item_id = item.get('id')
        title = item.get('title', '')[:30]

        valid, code = validate_item(item)

        if valid:
            valid_count += 1
            # is_verified = Trueë¡œ ì—…ë°ì´íŠ¸
            try:
                supabase.table(TABLE_COLLECTED_DATA)\
                    .update({'is_verified': True})\
                    .eq('id', item_id)\
                    .execute()
            except:
                pass
        else:
            # âœ… V30 ì¤‘ë³µ ìë™ ì œê±°: DUPLICATEì¸ ê²½ìš° ì¦‰ì‹œ ì‚­ì œ
            if code == "DUPLICATE":
                try:
                    # 1. evaluations ë¨¼ì € ì‚­ì œ (ìˆìœ¼ë©´)
                    eval_result = supabase.table(TABLE_EVALUATIONS)\
                        .delete()\
                        .eq('collected_data_id', item_id)\
                        .execute()
                    eval_deleted = len(eval_result.data) if eval_result.data else 0

                    # 2. collected_data ì‚­ì œ
                    supabase.table(TABLE_COLLECTED_DATA)\
                        .delete()\
                        .eq('id', item_id)\
                        .execute()

                    duplicate_removed += 1

                    # ë¡œê·¸ (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰)
                    if duplicate_removed % 10 == 0 or (i + 1) == len(items):
                        print(f"  ì¤‘ë³µ ì œê±° ì§„í–‰: {duplicate_removed}ê°œ (evaluations {eval_deleted}ê°œ í¬í•¨)")
                except Exception as e:
                    print(f"  âš ï¸ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨ ({item_id[:8]}...): {e}")
            else:
                # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¬ìˆ˜ì§‘ ëŒ€ìƒìœ¼ë¡œ ì¶”ê°€
                invalid_items.append({
                    'id': item_id,
                    'title': title,
                    'code': code,
                    'reason': VALIDATION_CODES.get(code, code),
                    'collector_ai': item.get('collector_ai'),
                    'category': item.get('category'),
                    'data_type': item.get('data_type'),
                    'source_url': item.get('source_url', '')[:50]
                })

        # ì§„í–‰ë¥  í‘œì‹œ
        if (i + 1) % 100 == 0:
            print(f"  ì§„í–‰: {i+1}/{len(items)} ({valid_count}ê°œ ìœ íš¨)")

    invalid_count = len(invalid_items)
    print(f"\nê²€ì¦ ì™„ë£Œ:")
    print(f"  âœ… ìœ íš¨: {valid_count}ê°œ")
    print(f"  ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {duplicate_removed}ê°œ")
    print(f"  âŒ ë¬´íš¨ (ì¬ìˆ˜ì§‘ í•„ìš”): {invalid_count}ê°œ")

    # ë¬´íš¨ í•­ëª© ìƒì„¸
    if invalid_items:
        print(f"\në¬´íš¨ í•­ëª© ìƒì„¸:")
        # ì½”ë“œë³„ ì§‘ê³„
        code_counts = {}
        for item in invalid_items:
            code = item['code']
            code_counts[code] = code_counts.get(code, 0) + 1

        for code, count in sorted(code_counts.items(), key=lambda x: -x[1]):
            print(f"  - {VALIDATION_CODES.get(code, code)}: {count}ê°œ")

        # AIë³„ ì§‘ê³„
        print(f"\nAIë³„ ë¬´íš¨ í•­ëª©:")
        ai_counts = {}
        for item in invalid_items:
            ai = item.get('collector_ai', 'unknown')
            ai_counts[ai] = ai_counts.get(ai, 0) + 1

        for ai, count in sorted(ai_counts.items(), key=lambda x: -x[1]):
            print(f"  - {ai}: {count}ê°œ")

    return {
        'total': len(items),
        'valid': valid_count,
        'invalid': invalid_count,
        'invalid_items': invalid_items
    }


def delete_invalid_items(invalid_items):
    """ë¬´íš¨ í•­ëª© ì‚­ì œ"""
    if not invalid_items:
        return 0

    deleted = 0
    for item in invalid_items:
        try:
            supabase.table(TABLE_COLLECTED_DATA)\
                .delete()\
                .eq('id', item['id'])\
                .execute()
            deleted += 1
        except Exception as e:
            print(f"  âš ï¸ ì‚­ì œ ì‹¤íŒ¨ ({item['id']}): {e}")

    print(f"  ğŸ—‘ï¸ {deleted}ê°œ ë¬´íš¨ í•­ëª© ì‚­ì œ")
    return deleted


def recollect_for_ai(ai_name, politician_id, politician_name, category, count, data_type):
    """íŠ¹ì • AIë¡œ ì¬ìˆ˜ì§‘"""
    print(f"  [{ai_name}] {category} {data_type} {count}ê°œ ì¬ìˆ˜ì§‘ ì¤‘...")

    # collect_v30ì˜ í•¨ìˆ˜ ì„í¬íŠ¸
    try:
        from collect_v30 import (
            init_ai_client, build_prompt_v30,
            call_perplexity, call_claude_with_websearch,
            call_gemini_with_search, call_grok,
            parse_json_response, AI_CONFIGS
        )
    except ImportError:
        print(f"  âŒ collect_v30.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    client = init_ai_client(ai_name)

    # 20-20-60 ë¶„ë°°
    neg = int(count * 0.2)
    pos = int(count * 0.2)
    neu = count - neg - pos
    sentiment_dist = {"negative": neg, "positive": pos, "neutral": neu}

    # ì¹´í…Œê³ ë¦¬ í•œê¸€ëª… ì°¾ê¸°
    cat_korean = next((k for c, k in CATEGORIES if c == category), category)

    prompt = build_prompt_v30(
        politician_name, category, cat_korean,
        data_type, count, sentiment_dist, ai_name
    )

    # AIë³„ í˜¸ì¶œ
    if ai_name == "Perplexity":
        result = call_perplexity(client, prompt, data_type)
    elif ai_name == "Claude":
        result = call_claude_with_websearch(client, prompt)
    elif ai_name == "Gemini":
        result = call_gemini_with_search(client, prompt)
    elif ai_name == "Grok":
        result = call_grok(client, prompt)
    else:
        result = None

    if not result:
        print(f"    âŒ ì¬ìˆ˜ì§‘ ì‹¤íŒ¨")
        return 0

    items = parse_json_response(result)

    # DB ì €ì¥
    saved = 0
    for item in items:
        try:
            record = {
                'politician_id': politician_id,
                'politician_name': politician_name,
                'category': category,
                'data_type': data_type,
                'collector_ai': ai_name.lower(),
                'title': item.get('data_title', '')[:200],
                'content': item.get('data_content', '')[:2000],
                'source_url': item.get('source_url', ''),
                'source_name': item.get('data_source', ''),
                'published_date': item.get('data_date'),
                'sentiment': item.get('sentiment', 'neutral'),
                'is_verified': False
            }
            supabase.table(TABLE_COLLECTED_DATA).insert(record).execute()
            saved += 1
        except Exception as e:
            pass

    print(f"    âœ… {saved}ê°œ ì¬ìˆ˜ì§‘ ì™„ë£Œ")
    return saved


def recollect_invalid(politician_id, politician_name, invalid_items):
    """ë¬´íš¨ í•­ëª© ì¬ìˆ˜ì§‘"""
    if not invalid_items:
        print("ì¬ìˆ˜ì§‘í•  í•­ëª© ì—†ìŒ")
        return 0

    print(f"\n{'='*60}")
    print(f"[ì¬ìˆ˜ì§‘] {len(invalid_items)}ê°œ í•­ëª©")
    print(f"{'='*60}")

    # AI/ì¹´í…Œê³ ë¦¬/data_typeë³„ ê·¸ë£¹í•‘
    groups = {}
    for item in invalid_items:
        key = (item['collector_ai'], item['category'], item['data_type'])
        if key not in groups:
            groups[key] = 0
        groups[key] += 1

    total_recollected = 0

    for (ai_name, category, data_type), count in groups.items():
        if ai_name and category and data_type:
            recollected = recollect_for_ai(
                ai_name.capitalize(),
                politician_id,
                politician_name,
                category,
                count,
                data_type
            )
            total_recollected += recollected
            time.sleep(1)

    print(f"\nì¬ìˆ˜ì§‘ ì™„ë£Œ: {total_recollected}ê°œ")
    return total_recollected


def run_validation_pipeline(politician_id, politician_name, mode='all', ai_name=None, max_iterations=3):
    """ê²€ì¦ + ì¬ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸"""
    print(f"\n{'#'*60}")
    print(f"# V30 ê²€ì¦ íŒŒì´í”„ë¼ì¸: {politician_name}")
    print(f"# ëª¨ë“œ: {mode}")
    print(f"{'#'*60}")

    if mode == 'validate':
        # ê²€ì¦ë§Œ
        result = validate_collected_data(politician_id, politician_name, ai_name)
        return result

    elif mode == 'recollect':
        # ì¬ìˆ˜ì§‘ë§Œ (is_verified=Falseì¸ í•­ëª©)
        items = get_collected_data(politician_id, ai_name)
        invalid_items = [
            {
                'id': item['id'],
                'collector_ai': item.get('collector_ai'),
                'category': item.get('category'),
                'data_type': item.get('data_type')
            }
            for item in items if not item.get('is_verified', False)
        ]

        if invalid_items:
            delete_invalid_items(invalid_items)
            recollect_invalid(politician_id, politician_name, invalid_items)

    else:  # mode == 'all'
        # ê²€ì¦ â†’ ì‚­ì œ â†’ ì¬ìˆ˜ì§‘ â†’ ì¬ê²€ì¦ (ë°˜ë³µ)
        for iteration in range(1, max_iterations + 1):
            print(f"\n{'='*60}")
            print(f"[ë°˜ë³µ {iteration}/{max_iterations}]")
            print(f"{'='*60}")

            # 1. ê²€ì¦
            result = validate_collected_data(politician_id, politician_name, ai_name)

            if result['invalid'] == 0:
                print(f"\nâœ… ëª¨ë“  ë°ì´í„° ìœ íš¨! ê²€ì¦ ì™„ë£Œ.")
                break

            # 2. ë¬´íš¨ í•­ëª© ì‚­ì œ
            delete_invalid_items(result['invalid_items'])

            # 3. ì¬ìˆ˜ì§‘
            recollect_invalid(politician_id, politician_name, result['invalid_items'])

            # ì ì‹œ ëŒ€ê¸°
            time.sleep(2)

        # ìµœì¢… ê²€ì¦
        print(f"\n{'='*60}")
        print(f"[ìµœì¢… ê²€ì¦]")
        print(f"{'='*60}")
        final_result = validate_collected_data(politician_id, politician_name, ai_name)

        return final_result


def main():
    parser = argparse.ArgumentParser(description='V30 ê²€ì¦ ë° ì¬ìˆ˜ì§‘')
    parser.add_argument('--politician_id', required=True, help='ì •ì¹˜ì¸ ID')
    parser.add_argument('--politician_name', required=True, help='ì •ì¹˜ì¸ ì´ë¦„')
    parser.add_argument('--mode', choices=['all', 'validate', 'recollect'], default='all',
                       help='ì‹¤í–‰ ëª¨ë“œ (all=ê²€ì¦+ì¬ìˆ˜ì§‘, validate=ê²€ì¦ë§Œ, recollect=ì¬ìˆ˜ì§‘ë§Œ)')
    parser.add_argument('--ai', choices=['Perplexity', 'Claude', 'Gemini', 'Grok'],
                       help='íŠ¹ì • AIë§Œ ê²€ì¦')
    parser.add_argument('--max_iterations', type=int, default=3,
                       help='ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 3)')

    args = parser.parse_args()

    run_validation_pipeline(
        args.politician_id,
        args.politician_name,
        mode=args.mode,
        ai_name=args.ai,
        max_iterations=args.max_iterations
    )


if __name__ == "__main__":
    main()
