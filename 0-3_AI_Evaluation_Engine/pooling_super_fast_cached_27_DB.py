"""
ì´ˆê³ ì† + ì´ˆì €ë¹„ìš© V24.5 Pooling í‰ê°€ ì‹œìŠ¤í…œ (DB ë²„ì „)
í”„ë¡¬í”„íŠ¸ ìºì‹±ìœ¼ë¡œ í† í° ì‚¬ìš©ëŸ‰ 90% ì ˆê°

í•µì‹¬ ì „ëµ:
1. collected_data í…Œì´ë¸”ì—ì„œ í‰ê°€ ë°ì´í„° ë¡œë“œ (OFFICIAL, PUBLIC, NEWS í¬í•¨)
2. Claude Prompt Caching (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + í‰ê°€ ë°ì´í„°)
3. ì²­í¬ ë°©ì‹ (30ê°œì”© 5ê°œ ì²­í¬)
4. ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ (3ê°œ AI ë™ì‹œ ì‹¤í–‰)

ì˜ˆìƒ ì„±ëŠ¥:
- ì‹œê°„: 1-2ì‹œê°„
- í† í°: ê¸°ì¡´ ëŒ€ë¹„ 90% ì ˆê°
"""

import asyncio
import aiohttp
import json
import os
import sys
from datetime import datetime
from typing import List, Dict
import anthropic
import openai
from supabase import create_client
from dotenv import load_dotenv

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROK_API_KEY = os.getenv("XAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

CATEGORIES = [
    "Expertise", "Leadership", "Vision", "Integrity", "Ethics",
    "Accountability", "Transparency", "Communication", "Responsiveness", "PublicInterest"
]

# ì •ì¹˜ì¸ ID ë§¤í•‘
POLITICIAN_IDS = {
    'ê¹€ë™ì—°': '17270f25',
    'ì˜¤ì„¸í›ˆ': '62e7b453',
    'í•œë™í›ˆ': '5516976b'
}


class CachedPoolingEvaluator:
    """í”„ë¡¬í”„íŠ¸ ìºì‹± ê¸°ë°˜ ì´ˆê³ ì† í‰ê°€ ì—”ì§„ (DB ë²„ì „)"""

    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        self.grok_api_key = GROK_API_KEY
        self.supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

        # í‰ê°€ ê¸°ì¤€ (ìºì‹±ìš©)
        self.evaluation_criteria = self._load_evaluation_criteria()

    def _load_evaluation_criteria(self) -> str:
        """í‰ê°€ ê¸°ì¤€ ë¡œë“œ (ìºì‹± ëŒ€ìƒ)"""
        return """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ 10ê°œ ê¸°ì¤€ìœ¼ë¡œ ì •ì¹˜ì¸ì„ í‰ê°€í•˜ì„¸ìš”:

1. **Expertise (ì „ë¬¸ì„±)**: ì •ì±… ì „ë¬¸ì„±, ë¶„ì•¼ ì§€ì‹, ê²½ë ¥
2. **Leadership (ë¦¬ë”ì‹­)**: ì¡°ì§ ê´€ë¦¬, ê²°ë‹¨ë ¥, ì¶”ì§„ë ¥
3. **Vision (ë¹„ì „)**: ë¯¸ë˜ ë¹„ì „, ì •ì±… ë°©í–¥ì„±, í˜ì‹ ì„±
4. **Integrity (ì²­ë ´ë„)**: ë¶€íŒ¨ ì—†ìŒ, ë„ë•ì„±, ì •ì§ì„±
5. **Ethics (ìœ¤ë¦¬ì„±)**: ìœ¤ë¦¬ì  íŒë‹¨, ê°€ì¹˜ê´€, ì›ì¹™
6. **Accountability (ì±…ì„ê°)**: ì•½ì† ì´í–‰, ì±…ì„ ì˜ì‹, ê²°ê³¼ ì±…ì„
7. **Transparency (íˆ¬ëª…ì„±)**: ì •ë³´ ê³µê°œ, íˆ¬ëª…í•œ ì˜ì‚¬ê²°ì •, ê³µê°œì„±
8. **Communication (ì†Œí†µëŠ¥ë ¥)**: êµ­ë¯¼ê³¼ì˜ ì†Œí†µ, ì„¤ëª… ëŠ¥ë ¥, ê²½ì²­
9. **Responsiveness (ëŒ€ì‘ì„±)**: ë¯¼ì› ëŒ€ì‘, ìœ„ê¸° ê´€ë¦¬, ì‹ ì†ì„±
10. **PublicInterest (ê³µìµì„±)**: ê³µê³µì˜ ì´ìµ ìš°ì„ , ì‚¬íšŒì  ê°€ì¹˜

í‰ê°€ ë°©ë²•:
- ê° ë‰´ìŠ¤ì—ì„œ ì •ì¹˜ì¸ì˜ í–‰ë™/ë°œì–¸ì„ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„
- ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ 0-100ì  ì ìˆ˜ ë¶€ì—¬
- ê¸ì •ì  í–‰ë™: ë†’ì€ ì ìˆ˜
- ë¶€ì •ì  í–‰ë™: ë‚®ì€ ì ìˆ˜
- ì¤‘ë¦½ì  ë‰´ìŠ¤: 50ì 

ì¶œë ¥ í˜•ì‹ (JSON):
{
  "scores": {
    "Expertise": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜N],
    "Leadership": [ì ìˆ˜1, ì ìˆ˜2, ..., ì ìˆ˜N],
    ... (10ê°œ ì¹´í…Œê³ ë¦¬)
  }
}
"""

    def _get_politician_id(self, politician_name: str) -> str:
        """ì •ì¹˜ì¸ ì´ë¦„ìœ¼ë¡œ ID ì¡°íšŒ"""
        if politician_name in POLITICIAN_IDS:
            return POLITICIAN_IDS[politician_name]

        # DBì—ì„œ ì¡°íšŒ
        response = self.supabase.table('politicians').select('id').eq('name', politician_name).execute()

        if response.data:
            politician_id = response.data[0]['id']
            POLITICIAN_IDS[politician_name] = politician_id
            return politician_id

        return None

    def _load_evaluation_data_from_db(self, politician_name: str) -> List[str]:
        """DBì—ì„œ í‰ê°€ ë°ì´í„° ë¡œë“œ (collected_data í…Œì´ë¸”)"""

        politician_id = self._get_politician_id(politician_name)

        if not politician_id:
            print(f"âš ï¸ {politician_name}: ì •ì¹˜ì¸ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []

        print(f"  ğŸ“‚ {politician_name} (ID: {politician_id}) ë°ì´í„° ë¡œë”©...")

        # collected_data í…Œì´ë¸”ì—ì„œ ì „ì²´ ë°ì´í„° ì¡°íšŒ
        response = self.supabase.table('collected_data').select(
            'data_title, data_content, ai_name, category_name, source_type'
        ).eq('politician_id', politician_id).execute()

        if not response.data:
            print(f"âš ï¸ {politician_name}: DBì— ë°ì´í„° ì—†ìŒ")
            return []

        print(f"  ğŸ“Š ì´ {len(response.data)}ê°œ í•­ëª© ë°œê²¬")

        # í‰ê°€ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì œëª© + ë‚´ìš©)
        data_list = []
        for item in response.data:
            title = item.get('data_title', '')
            content = item.get('data_content', '')
            ai = item.get('ai_name', 'Unknown')
            category = item.get('category_name', '')
            source_type = item.get('source_type', '')

            # í‰ê°€ ë°ì´í„° í…ìŠ¤íŠ¸ ìƒì„±
            data_text = f"[{ai} | {category} | {source_type}] {title}\n{content}"
            data_list.append(data_text)

        print(f"  âœ… {len(data_list)}ê°œ í‰ê°€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì „ì²´ ì‚¬ìš©)")
        return data_list

    def _chunk_data(self, data_list: List[str], chunk_size: int = 30) -> List[List[str]]:
        """í‰ê°€ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        return [data_list[i:i+chunk_size] for i in range(0, len(data_list), chunk_size)]

    async def evaluate_chunk_with_claude_cached(
        self,
        politician_name: str,
        data_chunk: List[str],
        chunk_num: int
    ) -> Dict:
        """Claudeë¡œ ì²­í¬ í‰ê°€ (í”„ë¡¬í”„íŠ¸ ìºì‹± í™œìš©)"""

        print(f"    ğŸ“Š Claude ì²­í¬ {chunk_num} í‰ê°€: {len(data_chunk)}ê°œ í•­ëª©")

        # í‰ê°€ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
        data_json = json.dumps(
            [{"index": i+1, "content": data} for i, data in enumerate(data_chunk)],
            ensure_ascii=False
        )

        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=8000,
                system=[
                    {
                        "type": "text",
                        "text": self.evaluation_criteria,
                        "cache_control": {"type": "ephemeral"}  # ìºì‹±!
                    }
                ],
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"ì •ì¹˜ì¸: {politician_name}\n\ní‰ê°€ ë°ì´í„° ëª©ë¡:\n{data_json}",
                                "cache_control": {"type": "ephemeral"}  # ë°ì´í„° ìºì‹±!
                            },
                            {
                                "type": "text",
                                "text": "ìœ„ í‰ê°€ ë°ì´í„°ë“¤ì„ ë¶„ì„í•˜ì—¬ 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
                            }
                        ]
                    }
                ]
            )

            result = json.loads(response.content[0].text)

            # í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
            usage = response.usage
            print(f"      í† í°: ì…ë ¥ {usage.input_tokens}, ì¶œë ¥ {usage.output_tokens}")
            if hasattr(usage, 'cache_creation_input_tokens') and usage.cache_creation_input_tokens:
                print(f"      ìºì‹œ ìƒì„±: {usage.cache_creation_input_tokens} í† í°")
            if hasattr(usage, 'cache_read_input_tokens') and usage.cache_read_input_tokens:
                print(f"      ìºì‹œ ì¬ì‚¬ìš©: {usage.cache_read_input_tokens} í† í° (ì ˆì•½!)")

            print(f"      âœ… Claude ì²­í¬ {chunk_num} ì™„ë£Œ")
            return result

        except Exception as e:
            print(f"      âŒ Claude ì²­í¬ {chunk_num} ì˜¤ë¥˜: {str(e)}")
            return None

    async def evaluate_chunk_with_chatgpt(
        self,
        politician_name: str,
        data_chunk: List[str],
        chunk_num: int
    ) -> Dict:
        """ChatGPTë¡œ ì²­í¬ í‰ê°€"""

        print(f"    ğŸ“Š ChatGPT ì²­í¬ {chunk_num} í‰ê°€: {len(data_chunk)}ê°œ í•­ëª©")

        data_json = json.dumps(
            [{"index": i+1, "content": data} for i, data in enumerate(data_chunk)],
            ensure_ascii=False
        )

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": self.evaluation_criteria},
                    {"role": "user", "content": f"""
ì •ì¹˜ì¸: {politician_name}

í‰ê°€ ë°ì´í„° ëª©ë¡:
{data_json}

ìœ„ í‰ê°€ ë°ì´í„°ë“¤ì„ ë¶„ì„í•˜ì—¬ 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
                    """}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            print(f"      âœ… ChatGPT ì²­í¬ {chunk_num} ì™„ë£Œ")
            return result

        except Exception as e:
            print(f"      âŒ ChatGPT ì²­í¬ {chunk_num} ì˜¤ë¥˜: {str(e)}")
            return None

    async def evaluate_chunk_with_grok(
        self,
        politician_name: str,
        data_chunk: List[str],
        chunk_num: int
    ) -> Dict:
        """Grokìœ¼ë¡œ ì²­í¬ í‰ê°€"""

        print(f"    ğŸ“Š Grok ì²­í¬ {chunk_num} í‰ê°€: {len(data_chunk)}ê°œ í•­ëª©")

        data_json = json.dumps(
            [{"index": i+1, "content": data} for i, data in enumerate(data_chunk)],
            ensure_ascii=False
        )

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.grok_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "grok-beta",
                        "messages": [
                            {"role": "system", "content": self.evaluation_criteria},
                            {"role": "user", "content": f"""
ì •ì¹˜ì¸: {politician_name}

í‰ê°€ ë°ì´í„° ëª©ë¡:
{data_json}

ìœ„ í‰ê°€ ë°ì´í„°ë“¤ì„ ë¶„ì„í•˜ì—¬ 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
                            """}
                        ],
                        "temperature": 0.3
                    }
                ) as response:
                    data = await response.json()
                    result = json.loads(data['choices'][0]['message']['content'])
                    print(f"      âœ… Grok ì²­í¬ {chunk_num} ì™„ë£Œ")
                    return result

        except Exception as e:
            print(f"      âŒ Grok ì²­í¬ {chunk_num} ì˜¤ë¥˜: {str(e)}")
            return None

    async def evaluate_one_politician_chunked(self, politician_name: str) -> Dict:
        """í•œ ëª…ì˜ ì •ì¹˜ì¸ì„ ì²­í¬ ë°©ì‹ìœ¼ë¡œ í‰ê°€"""

        print(f"\nğŸ¯ í‰ê°€ ì‹œì‘: {politician_name}")

        # DBì—ì„œ í‰ê°€ ë°ì´í„° ë¡œë“œ
        data_list = self._load_evaluation_data_from_db(politician_name)

        if not data_list:
            print(f"âŒ {politician_name}: í‰ê°€ ë°ì´í„° ì—†ìŒ")
            return None

        # ì²­í¬ë¡œ ë¶„í•  (30ê°œì”©)
        data_chunks = self._chunk_data(data_list, chunk_size=30)
        print(f"  ğŸ“¦ {len(data_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        # ê° ì²­í¬ë³„ ê²°ê³¼ ì €ì¥
        all_chatgpt_scores = {cat: [] for cat in CATEGORIES}
        all_grok_scores = {cat: [] for cat in CATEGORIES}
        all_claude_scores = {cat: [] for cat in CATEGORIES}

        # ì²­í¬ë³„ í‰ê°€
        for i, chunk in enumerate(data_chunks, 1):
            print(f"\n  ğŸ“¦ ì²­í¬ {i}/{len(data_chunks)} í‰ê°€ ì¤‘...")

            # 3ê°œ AI ë™ì‹œ í‰ê°€
            chatgpt_task = self.evaluate_chunk_with_chatgpt(politician_name, chunk, i)
            grok_task = self.evaluate_chunk_with_grok(politician_name, chunk, i)
            claude_task = self.evaluate_chunk_with_claude_cached(politician_name, chunk, i)

            chatgpt_result, grok_result, claude_result = await asyncio.gather(
                chatgpt_task, grok_task, claude_task
            )

            # ê²°ê³¼ ëˆ„ì 
            if chatgpt_result and 'scores' in chatgpt_result:
                for cat in CATEGORIES:
                    all_chatgpt_scores[cat].extend(chatgpt_result['scores'].get(cat, []))

            if grok_result and 'scores' in grok_result:
                for cat in CATEGORIES:
                    all_grok_scores[cat].extend(grok_result['scores'].get(cat, []))

            if claude_result and 'scores' in claude_result:
                for cat in CATEGORIES:
                    all_claude_scores[cat].extend(claude_result['scores'].get(cat, []))

        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê³„ì‚°
        chatgpt_avg = {cat: sum(scores)/len(scores) if scores else 0
                       for cat, scores in all_chatgpt_scores.items()}
        grok_avg = {cat: sum(scores)/len(scores) if scores else 0
                    for cat, scores in all_grok_scores.items()}
        claude_avg = {cat: sum(scores)/len(scores) if scores else 0
                      for cat, scores in all_claude_scores.items()}

        # Pooling ì ìˆ˜ ê³„ì‚°
        pooling_scores = {}
        for cat in CATEGORIES:
            pooling_scores[cat] = (chatgpt_avg[cat] + grok_avg[cat] + claude_avg[cat]) / 3

        print(f"\nâœ… {politician_name} í‰ê°€ ì™„ë£Œ!")
        print(f"  í‰ê°€ ë°ì´í„° ìˆ˜: {len(data_list)}ê°œ")
        print(f"  Pooling í‰ê· : {sum(pooling_scores.values())/len(pooling_scores):.2f}ì ")

        return {
            'politician': politician_name,
            'chatgpt_avg': chatgpt_avg,
            'grok_avg': grok_avg,
            'claude_avg': claude_avg,
            'pooling': pooling_scores,
            'data_count': len(data_list)
        }

    async def evaluate_batch(self, politicians: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ ì •ì¹˜ì¸ ë³‘ë ¬ í‰ê°€"""

        print(f"\n{'='*60}")
        print(f"ğŸ“¦ ë°°ì¹˜ í‰ê°€ ì‹œì‘: {len(politicians)}ëª…")
        print(f"{'='*60}")

        tasks = [self.evaluate_one_politician_chunked(p) for p in politicians]
        results = await asyncio.gather(*tasks)

        return [r for r in results if r is not None]

    async def evaluate_all_27(self, politician_names: List[str]) -> List[Dict]:
        """27ëª… ì „ì²´ í‰ê°€ (3ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• )"""

        print("\n" + "="*60)
        print("ğŸš€ ì´ˆê³ ì† V24.5 Pooling í‰ê°€ ì‹œì‘ (DB ë²„ì „)")
        print("="*60)
        print(f"ëŒ€ìƒ: {len(politician_names)}ëª… ì •ì¹˜ì¸")
        print(f"ë°©ì‹: 3ê°œ ê·¸ë£¹ Ã— ë³‘ë ¬ ì²˜ë¦¬")
        print(f"ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„")
        print("="*60 + "\n")

        # 3ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„í•  (API Rate Limit ê³ ë ¤)
        group_size = (len(politician_names) + 2) // 3  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
        groups = [
            politician_names[i:i+group_size]
            for i in range(0, len(politician_names), group_size)
        ]

        all_results = []
        start_time = datetime.now()

        for i, group in enumerate(groups, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“¦ ê·¸ë£¹ {i}/{len(groups)} ì‹œì‘ ({len(group)}ëª…)")
            print(f"{'='*60}")

            group_start = datetime.now()
            group_results = await self.evaluate_batch(group)
            group_elapsed = (datetime.now() - group_start).total_seconds() / 60

            all_results.extend(group_results)

            # ì¦‰ì‹œ ì €ì¥
            self._save_batch_results(group_results, group_num=i)

            print(f"\nâœ… ê·¸ë£¹ {i} ì™„ë£Œ!")
            print(f"  ì†Œìš” ì‹œê°„: {group_elapsed:.1f}ë¶„")
            print(f"  ì „ì²´ ì§„í–‰ë¥ : {len(all_results)}/{len(politician_names)}ëª… ({len(all_results)/len(politician_names)*100:.1f}%)")

        total_elapsed = (datetime.now() - start_time).total_seconds() / 60

        print(f"\n{'='*60}")
        print(f"ğŸ‰ ì „ì²´ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"í‰ê°€ ì™„ë£Œ: {len(all_results)}/{len(politician_names)}ëª…")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.1f}ë¶„ ({total_elapsed/60:.2f}ì‹œê°„)")
        print(f"{'='*60}\n")

        return all_results

    def _save_batch_results(self, results: List[Dict], group_num: int):
        """ë°°ì¹˜ ê²°ê³¼ ì €ì¥"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"pooling_results_group{group_num}_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    evaluator = CachedPoolingEvaluator()

    # DBì—ì„œ ì •ì¹˜ì¸ ëª©ë¡ ì¡°íšŒ
    response = evaluator.supabase.table('politicians').select('name').execute()

    if not response.data:
        print("âŒ DBì— ì •ì¹˜ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    all_politicians = [p['name'] for p in response.data]

    print(f"\nğŸ“‹ DBì—ì„œ {len(all_politicians)}ëª…ì˜ ì •ì¹˜ì¸ ë°œê²¬:")
    for i, name in enumerate(all_politicians, 1):
        print(f"  {i}. {name}")

    # ì´ë¯¸ ì™„ë£Œëœ 3ëª… ì œì™¸
    completed = ['ê¹€ë™ì—°', 'ì˜¤ì„¸í›ˆ', 'í•œë™í›ˆ']
    remaining = [p for p in all_politicians if p not in completed]

    print(f"\nâœ… ì´ë¯¸ ì™„ë£Œ: {len(completed)}ëª… ({', '.join(completed)})")
    print(f"â³ ë‚¨ì€ ì‘ì—…: {len(remaining)}ëª…")

    if not remaining:
        print("\nâœ… ëª¨ë“  ì •ì¹˜ì¸ í‰ê°€ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return

    # í‰ê°€ ì‹¤í–‰
    results = await evaluator.evaluate_all_27(remaining)

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_filename = f"pooling_results_all_{len(remaining)}_{timestamp}.json"

    with open(final_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ìµœì¢… ê²°ê³¼ ì €ì¥: {final_filename}")

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š í‰ê°€ í†µê³„:")
    for result in results:
        politician = result['politician']
        pooling = result['pooling']
        avg_score = sum(pooling.values()) / len(pooling)
        print(f"  {politician}: {avg_score:.2f}ì  ({result['data_count']}ê°œ í‰ê°€ ë°ì´í„°)")


if __name__ == "__main__":
    asyncio.run(main())
